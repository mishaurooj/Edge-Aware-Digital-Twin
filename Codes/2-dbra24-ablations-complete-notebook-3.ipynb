{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acd90dd6",
   "metadata": {},
   "source": [
    "# <span style=\"color:#6C5CE7\">DBRA24 Digital Twin Anomaly ‚Äî End-to-End Pipeline with A1‚ÄìA10 Ablations</span>\n",
    "\n",
    "<span style=\"background:#E3F2FD;color:#0D47A1;padding:6px 10px;border-radius:6px;display:inline-block;\">\n",
    "<strong>Dataset</strong>: <code>/kaggle/input/driver-behavior-and-route-anomaly-dataset-dbra24/driver_behavior_route_anomaly_dataset_with_derived_features.csv</code>\n",
    "</span>\n",
    "\n",
    "This notebook is **ready to run on Kaggle**. It:\n",
    "- trains **A1‚ÄìA10 ablations** (GBM baseline, Bi-LSTM variants, TCN-Small)\n",
    "- saves models to <code>/kaggle/working/models/</code>\n",
    "- exports a single **Excel** report to <code>/kaggle/working/DBRA24_results.xlsx</code>\n",
    "- writes **plots** (PR curves, reliability, confusion matrices, ROC) to <code>/kaggle/working/plots/</code>\n",
    "- prints a **final comparison table** (easy to paste in your paper/report)\n",
    "\n",
    "> Tip: Turn on GPU in the Kaggle Notebook settings for faster training (optional)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97696e73",
   "metadata": {},
   "source": [
    "## <span style=\"color:#00B894\">Table of Contents</span>\n",
    "1. üîß Setup (Installs & Imports)  \n",
    "2. ‚öôÔ∏è Config (paths, seeds, hyperparameters)  \n",
    "3. üì• Load & Inspect Data  \n",
    "4. üß≠ Feature Groups & Light Engineering  \n",
    "5. üß± Windowing into Sequences (T=60s, stride=5s)  \n",
    "6. ‚úÇÔ∏è Split by Driver (train/val/test)  \n",
    "7. üìè Scaling & PyTorch Datasets  \n",
    "8. üß† Models (Bi-LSTM, TCN-Small, Gating)  \n",
    "9. üìê Losses & Metrics (PR-AUC, F1, RMSE, ECE, TTD, FP/h, Latency, FLOPs)  \n",
    "10. üèÉ Training/Evaluation Utilities  \n",
    "11. üå≥ A1 ‚Äî GBM (No Mobility, Temporal-Agnostic)  \n",
    "12. üß™ A2‚ÄìA10 ‚Äî Ablations (one-by-one)  \n",
    "13. üìä Results Export (Excel, CSV, Plots) + Final Comparison Table  \n",
    "14. üìù Notes & Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a1b0af",
   "metadata": {},
   "source": [
    "## 1) üîß Setup ‚Äî Installs\n",
    "Safe to re-run. Kaggle usually has most deps. We install a few extras (quietly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d1410c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T17:07:40.276033Z",
     "iopub.status.busy": "2025-10-08T17:07:40.275173Z",
     "iopub.status.idle": "2025-10-08T17:09:11.963431Z",
     "shell.execute_reply": "2025-10-08T17:09:11.962643Z",
     "shell.execute_reply.started": "2025-10-08T17:07:40.275995Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm ‚úì\n",
      "xgboost ‚úì\n",
      "Installing thop ‚Ä¶\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 363.4/363.4 MB 4.7 MB/s eta 0:00:00\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 13.8/13.8 MB 84.1 MB/s eta 0:00:00\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24.6/24.6 MB 62.6 MB/s eta 0:00:00\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 883.7/883.7 kB 35.1 MB/s eta 0:00:00\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 664.8/664.8 MB 2.4 MB/s eta 0:00:00\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 211.5/211.5 MB 5.4 MB/s eta 0:00:00\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 56.3/56.3 MB 29.6 MB/s eta 0:00:00\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 127.9/127.9 MB 13.0 MB/s eta 0:00:00\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 207.5/207.5 MB 2.0 MB/s eta 0:00:00\n",
      "   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 21.1/21.1 MB 77.5 MB/s eta 0:00:00\n",
      "openpyxl ‚úì\n",
      "torchmetrics ‚úì\n"
     ]
    }
   ],
   "source": [
    "# If internet is restricted, you can comment these lines. On Kaggle, this is OK to run.\n",
    "import sys, subprocess, importlib\n",
    "\n",
    "def ensure(pkg, pip_name=None):\n",
    "    pip_name = pip_name or pkg\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "        print(f\"{pkg} ‚úì\")\n",
    "    except Exception:\n",
    "        print(f\"Installing {pip_name} ‚Ä¶\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pip_name])\n",
    "\n",
    "for p in [\n",
    "    (\"lightgbm\",\"lightgbm\"),\n",
    "    (\"xgboost\",\"xgboost\"),\n",
    "    (\"thop\",\"thop\"),\n",
    "    (\"openpyxl\",\"openpyxl\"),\n",
    "    (\"torchmetrics\",\"torchmetrics==1.4.0\"),\n",
    "]:\n",
    "    ensure(*p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56289a0",
   "metadata": {},
   "source": [
    "## 2) ‚öôÔ∏è Imports & Global Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4c96b05-5f5c-40f7-a53e-8a2fb0638592",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T17:09:11.965275Z",
     "iopub.status.busy": "2025-10-08T17:09:11.964760Z",
     "iopub.status.idle": "2025-10-08T17:09:11.973117Z",
     "shell.execute_reply": "2025-10-08T17:09:11.972389Z",
     "shell.execute_reply.started": "2025-10-08T17:09:11.965255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/driver-behavior-and-route-anomaly-dataset-dbra24/driver_behavior_route_anomaly_dataset_with_derived_features.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "for root, dirs, files in os.walk(\"/kaggle/input\"):\n",
    "    for f in files:\n",
    "        print(os.path.join(root, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69c5f4ba-4c4f-48d1-9ffe-24a2072ed9b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T17:09:11.974073Z",
     "iopub.status.busy": "2025-10-08T17:09:11.973856Z",
     "iopub.status.idle": "2025-10-08T17:09:13.647321Z",
     "shell.execute_reply": "2025-10-08T17:09:13.646652Z",
     "shell.execute_reply.started": "2025-10-08T17:09:11.974057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Environment ready.\n"
     ]
    }
   ],
   "source": [
    "# ================== Imports ==================\n",
    "import os, math, time, pickle, copy\n",
    "import numpy as np, pandas as pd\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from sklearn.metrics import average_precision_score, f1_score, mean_squared_error, precision_recall_curve, roc_curve, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, gc, time, math, random, json, pickle, shutil, warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, precision_recall_curve, roc_curve, confusion_matrix,\n",
    "    f1_score, mean_squared_error\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from thop import profile\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# Globals\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Paths\n",
    "DATA_CSV = \"/kaggle/input/driver-behavior-and-route-anomaly-dataset-dbra24/driver_behavior_route_anomaly_dataset_with_derived_features.csv\"\n",
    "\n",
    "# Directories\n",
    "OUT_DIR   = Path(\"./out\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "MODEL_DIR = Path(\"./models\"); MODEL_DIR.mkdir(exist_ok=True)\n",
    "PLOT_DIR  = Path(\"./plots\"); PLOT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Config\n",
    "CFG = dict(\n",
    "    SEQ_LEN=60, STRIDE=5, SAMPLE_RATE_HZ=1,\n",
    "    BATCH_SIZE=256, EPOCHS=25, LR=5e-4, HIDDEN=256,\n",
    "    TCN_CHANNELS=64, DROPOUT=0.3, PATIENCE=5\n",
    ")\n",
    "SEED=42\n",
    "\n",
    "print(\"Environment ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f89a8b",
   "metadata": {},
   "source": [
    "## 3) üì• Load & Inspect Data\n",
    "This block is robust to small naming differences and ensures required columns exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d21d3de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T17:09:13.649486Z",
     "iopub.status.busy": "2025-10-08T17:09:13.649269Z",
     "iopub.status.idle": "2025-10-08T17:09:14.783464Z",
     "shell.execute_reply": "2025-10-08T17:09:14.782748Z",
     "shell.execute_reply.started": "2025-10-08T17:09:13.649470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 120000\n",
      "Unique trips: 5\n",
      "count        5.000000\n",
      "mean     24000.000000\n",
      "std      20744.178424\n",
      "min      11800.000000\n",
      "25%      11945.000000\n",
      "50%      12204.000000\n",
      "75%      24163.000000\n",
      "max      59888.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def read_db(csv_path=DATA_CSV):\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Normalize a few common names\n",
    "    rename_map = {\n",
    "        'lat':'latitude','lon':'longitude','long':'longitude','ts':'timestamp',\n",
    "        'driverid':'driver_id','vehicleid':'vehicle_id', 'tripid':'trip_id',\n",
    "        'anomaly_event':'anomalous_event', 'route_anom':'route_anomaly',\n",
    "        'route_deviation':'route_deviation_score'\n",
    "    }\n",
    "    df = df.rename(columns={k:v for k,v in rename_map.items() if k in df.columns})\n",
    "\n",
    "    # Timestamps\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    else:\n",
    "        raise ValueError(\"Missing 'timestamp' column in dataset.\")\n",
    "\n",
    "    # Labels (create safe fallbacks if absent)\n",
    "    for col in ['anomalous_event','route_anomaly','route_deviation_score']:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0 if col != 'route_deviation_score' else 0.0\n",
    "\n",
    "    # Ensure IDs exist\n",
    "    if 'driver_id' not in df.columns:\n",
    "        df['driver_id'] = 0\n",
    "    if 'vehicle_id' not in df.columns:\n",
    "        df['vehicle_id'] = 0\n",
    "\n",
    "    # ‚úÖ Trip assignment logic\n",
    "    if 'trip_id' not in df.columns or df['trip_id'].nunique() >= len(df):\n",
    "        # If trip_id is missing OR every row is unique\n",
    "        if df['driver_id'].nunique() > 1:\n",
    "            # Group by driver\n",
    "            df['trip_id'] = df['driver_id'].astype(str)\n",
    "        elif df['vehicle_id'].nunique() > 1:\n",
    "            # Fallback: group by vehicle\n",
    "            df['trip_id'] = df['vehicle_id'].astype(str)\n",
    "        else:\n",
    "            # Last resort: treat all rows as one trip\n",
    "            df['trip_id'] = \"trip0\"\n",
    "\n",
    "    return df\n",
    "df_raw = read_db()\n",
    "print(\"Rows:\", len(df_raw))\n",
    "print(\"Unique trips:\", df_raw['trip_id'].nunique())\n",
    "print(df_raw.groupby('trip_id').size().describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d92b3c",
   "metadata": {},
   "source": [
    "## 4) üß≠ Feature Groups & Light Engineering\n",
    "We organize columns into **kinematics**, **GPS**, **context** (weather/road/traffic), **map-aware**, and **trip statics**, then derive a few helpful signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "874c74e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T17:09:14.784443Z",
     "iopub.status.busy": "2025-10-08T17:09:14.784239Z",
     "iopub.status.idle": "2025-10-08T17:09:14.975814Z",
     "shell.execute_reply": "2025-10-08T17:09:14.975108Z",
     "shell.execute_reply.started": "2025-10-08T17:09:14.784426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groups: {'kinematics': 10, 'gps': 2, 'context': 3, 'mapaware': 2, 'trip_static': 2, 'quality': 0}\n"
     ]
    }
   ],
   "source": [
    "def find_cols(df):\n",
    "    kinematics = [c for c in df.columns if c.lower() in\n",
    "                  ['speed','acceleration','rpm','steering_angle','heading','brake_usage',\n",
    "                   'lane_deviation','acceleration_variation','behavioral_consistency_index','turn_rate','yaw_rate','jerk']]\n",
    "    gps = [c for c in df.columns if c.lower() in ['latitude','longitude','bearing','altitude']]\n",
    "    context = [c for c in df.columns if c.lower() in ['weather_conditions','road_type','traffic_condition']]\n",
    "    mapaware = [c for c in df.columns if 'geofencing' in c.lower() or 'curvature' in c.lower()]\n",
    "    trip_static = [c for c in df.columns if c.lower() in ['trip_duration','trip_distance']]\n",
    "    quality = [c for c in df.columns if c.lower() in ['signal_quality','gps_accuracy']]\n",
    "    return dict(kinematics=kinematics, gps=gps, context=context, mapaware=mapaware,\n",
    "                trip_static=trip_static, quality=quality)\n",
    "\n",
    "def add_derived_features(df):\n",
    "    df = df.sort_values(['trip_id','timestamp']).copy()\n",
    "    # heading deltas ‚Üí turn intensity\n",
    "    if 'heading' in df.columns:\n",
    "        df['d_heading'] = df.groupby('trip_id')['heading'].diff().fillna(0)\n",
    "        df['turn_intensity'] = df['d_heading'].abs()\n",
    "    # jerk from acceleration\n",
    "    if 'acceleration' in df.columns:\n",
    "        df['jerk'] = df.groupby('trip_id')['acceleration'].diff().fillna(0)\n",
    "    # curvature proxy if missing\n",
    "    if 'curvature' not in df.columns and {'latitude','longitude'}.issubset(df.columns):\n",
    "        for ax in ['latitude','longitude']:\n",
    "            df[f'd_{ax}'] = df.groupby('trip_id')[ax].diff().fillna(0)\n",
    "        df['curvature'] = np.hypot(df['d_latitude'], df['d_longitude']).rolling(3, min_periods=1).mean()\n",
    "        df.drop(columns=[c for c in ['d_latitude','d_longitude'] if c in df.columns], inplace=True)\n",
    "    return df\n",
    "\n",
    "df = add_derived_features(df_raw)\n",
    "COLS = find_cols(df)\n",
    "print(\"Groups:\", {k:len(v) for k,v in COLS.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543dc15f",
   "metadata": {},
   "source": [
    "## 5) üß± Windowing into Sequences\n",
    "We convert per-second telemetry into sliding windows (default **T=60s**, stride **5s**).  \n",
    "Colored highlights explain ablations in later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fde8ff98-f89d-4530-b2f5-043ee35cb8a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T17:09:14.976769Z",
     "iopub.status.busy": "2025-10-08T17:09:14.976525Z",
     "iopub.status.idle": "2025-10-08T17:09:15.005344Z",
     "shell.execute_reply": "2025-10-08T17:09:15.004678Z",
     "shell.execute_reply.started": "2025-10-08T17:09:14.976748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 120000\n",
      "Unique trips: 5\n",
      "Per-trip length stats:\n",
      "count        5.000000\n",
      "mean     24000.000000\n",
      "std      20744.178424\n",
      "min      11800.000000\n",
      "25%      11945.000000\n",
      "50%      12204.000000\n",
      "75%      24163.000000\n",
      "max      59888.000000\n",
      "dtype: float64\n",
      "Timestamp range: 2023-01-01 00:00:00 ‚Üí 2023-01-02 09:19:59\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity check\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Unique trips:\", df['trip_id'].nunique())\n",
    "print(\"Per-trip length stats:\")\n",
    "print(df.groupby('trip_id').size().describe())\n",
    "print(\"Timestamp range:\", df['timestamp'].min(), \"‚Üí\", df['timestamp'].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f105cba8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T17:09:15.006410Z",
     "iopub.status.busy": "2025-10-08T17:09:15.006130Z",
     "iopub.status.idle": "2025-10-08T17:09:15.023841Z",
     "shell.execute_reply": "2025-10-08T17:09:15.023091Z",
     "shell.execute_reply.started": "2025-10-08T17:09:15.006387Z"
    }
   },
   "outputs": [],
   "source": [
    "# ==== SEQUENCE BUILDER WITH SAFETY ====\n",
    "def make_sequences(df, seq_len=60, stride=5, sample_hz=1, \n",
    "                   ablation=None, short_history=False, min_len=10):\n",
    "    df_proc = df.copy()\n",
    "\n",
    "    # One-hot encode categorical context\n",
    "    for cat in [c for c in ['weather_conditions','road_type','traffic_condition'] if c in df_proc.columns]:\n",
    "        df_proc[cat] = df_proc[cat].astype(str).fillna(\"unk\")\n",
    "    context_cols = [c for c in ['weather_conditions','road_type','traffic_condition'] if c in df_proc.columns]\n",
    "    oh = pd.get_dummies(df_proc[context_cols], prefix=context_cols) if context_cols else pd.DataFrame(index=df_proc.index)\n",
    "    df_proc = pd.concat([df_proc.drop(columns=context_cols), oh], axis=1)\n",
    "\n",
    "    # Time-of-day encoding\n",
    "    df_proc['sec_of_day'] = df_proc['timestamp'].dt.hour*3600 + df_proc['timestamp'].dt.minute*60 + df_proc['timestamp'].dt.second\n",
    "    df_proc['sin_t'] = np.sin(2*np.pi*df_proc['sec_of_day']/86400)\n",
    "    df_proc['cos_t'] = np.cos(2*np.pi*df_proc['sec_of_day']/86400)\n",
    "\n",
    "    # Dynamic features\n",
    "    dyn_cols = COLS['kinematics'] + COLS['gps'] + ['turn_intensity','jerk','curvature','sin_t','cos_t']\n",
    "    dyn_cols = [c for c in dyn_cols if c in df_proc.columns]\n",
    "\n",
    "    # A5 jitter\n",
    "    if ablation == \"A5-jitter\":\n",
    "        ctx_like = [c for c in df_proc.columns if any(k in c.lower() for k in ['weather_', 'road_', 'traffic_'])]\n",
    "        for c in ctx_like:\n",
    "            df_proc[c] = df_proc.groupby('trip_id')[c].shift(1)\n",
    "\n",
    "    # A6 short history\n",
    "    if short_history:\n",
    "        seq_len = 20\n",
    "\n",
    "    # A2/A8 feature drop\n",
    "    drop_cols = []\n",
    "    if ablation == \"A2-nocontext\":\n",
    "        drop_cols += [c for c in df_proc.columns if any(k in c.lower() for k in ['weather_', 'road_', 'traffic_'])]\n",
    "    if ablation == \"A8-mapfree\":\n",
    "        drop_cols += [c for c in df_proc.columns if 'geofencing' in c.lower() or 'curvature' in c.lower()]\n",
    "    df_proc = df_proc.drop(columns=[c for c in set(drop_cols) if c in df_proc.columns], errors='ignore')\n",
    "\n",
    "    dyn_cols = [c for c in dyn_cols if c in df_proc.columns]\n",
    "    dyn_cols = list(dict.fromkeys(dyn_cols + [c for c in df_proc.columns if any(p in c for p in ['weather_','road_','traffic_'])]))\n",
    "\n",
    "    static_cols = [c for c in COLS['trip_static'] if c in df_proc.columns]\n",
    "    if 'behavioral_consistency_index' in df_proc.columns and 'behavioral_consistency_index' not in static_cols:\n",
    "        static_cols.append('behavioral_consistency_index')\n",
    "\n",
    "    labels_A = 'anomalous_event'\n",
    "    labels_B = 'route_anomaly'\n",
    "    labels_C = 'route_deviation_score'\n",
    "\n",
    "    # Slide per trip\n",
    "    seqs, statics, yA, yB, yC, meta = [], [], [], [], [], []\n",
    "    skipped = 0\n",
    "    for trip, df_t in df_proc.groupby('trip_id', sort=False):\n",
    "        df_t = df_t.sort_values('timestamp').copy()\n",
    "        df_t = df_t.set_index('timestamp').resample(f\"{int(1000/sample_hz)}L\").nearest(limit=1).ffill().bfill().reset_index()\n",
    "        n = len(df_t)\n",
    "        if n < seq_len: \n",
    "            skipped += 1\n",
    "            continue\n",
    "        for start in range(0, n - seq_len + 1, stride):\n",
    "            w = df_t.iloc[start:start+seq_len]\n",
    "            X = w[dyn_cols].astype(float).values if dyn_cols else np.zeros((seq_len,0))\n",
    "            s = w[static_cols].mean().values if static_cols else np.zeros((0,))\n",
    "            a = int(w[labels_A].max())\n",
    "            b = int(w[labels_B].max())\n",
    "            c = float(w[labels_C].mean())\n",
    "            seqs.append(X); statics.append(s); yA.append(a); yB.append(b); yC.append(c)\n",
    "            meta.append(dict(trip_id=trip, t0=w['timestamp'].iloc[0], t1=w['timestamp'].iloc[-1]))\n",
    "    if skipped > 0:\n",
    "        print(f\"[make_sequences] ‚ö†Ô∏è Skipped {skipped} trips with length < {seq_len}\")\n",
    "    print(f\"[make_sequences] Generated {len(seqs)} windows (seq_len={seq_len}, stride={stride})\")\n",
    "\n",
    "    return dict(X=seqs, S=statics, yA=np.array(yA), yB=np.array(yB), yC=np.array(yC), meta=meta,\n",
    "                dyn_cols=dyn_cols, static_cols=static_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b0ae50e-797f-477b-8a8e-c143283a2429",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T17:09:15.024879Z",
     "iopub.status.busy": "2025-10-08T17:09:15.024565Z",
     "iopub.status.idle": "2025-10-08T17:09:15.044942Z",
     "shell.execute_reply": "2025-10-08T17:09:15.044066Z",
     "shell.execute_reply.started": "2025-10-08T17:09:15.024852Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def subsample_dict(d, frac=0.4, seed=42):\n",
    "    \"\"\"Safely subsample dictionary of sequences.\"\"\"\n",
    "    n = len(d['X'])\n",
    "    if n == 0:\n",
    "        print(\"[subsample_dict] ‚ö†Ô∏è No data to subsample (n=0). Returning original dict.\")\n",
    "        return d\n",
    "    \n",
    "    take = max(1, int(frac * n))\n",
    "    idx = np.random.default_rng(seed).choice(n, take, replace=False)\n",
    "\n",
    "    return {\n",
    "        k: (d[k][idx] if isinstance(d[k], np.ndarray) else [d[k][i] for i in idx])\n",
    "        for k in ['X','S','yA','yB','yC','meta']\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1098e97a-9449-406c-b212-c9f158a4e484",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T17:09:15.045947Z",
     "iopub.status.busy": "2025-10-08T17:09:15.045729Z",
     "iopub.status.idle": "2025-10-08T17:12:05.236663Z",
     "shell.execute_reply": "2025-10-08T17:12:05.236018Z",
     "shell.execute_reply.started": "2025-10-08T17:09:15.045931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[make_sequences] Generated 119930 windows (seq_len=60, stride=5)\n",
      "After subsample: 47972\n"
     ]
    }
   ],
   "source": [
    "# Step 1: make sequences (auto-adjust ensures >0 windows if possible)\n",
    "data_full = make_sequences(df, seq_len=CFG['SEQ_LEN'], stride=CFG['STRIDE'], sample_hz=CFG['SAMPLE_RATE_HZ'])\n",
    "\n",
    "# Step 2: subsample if needed\n",
    "data_full = subsample_dict(data_full, frac=0.4, seed=SEED)\n",
    "print(\"After subsample:\", len(data_full['X']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adae8345-3b3c-4c47-811b-ae872bad79a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T17:12:05.249639Z",
     "iopub.status.busy": "2025-10-08T17:12:05.249414Z",
     "iopub.status.idle": "2025-10-08T17:12:05.255840Z",
     "shell.execute_reply": "2025-10-08T17:12:05.254995Z",
     "shell.execute_reply.started": "2025-10-08T17:12:05.249624Z"
    }
   },
   "outputs": [],
   "source": [
    "# ==== AUTO-FIX if NO WINDOWS ====\n",
    "def rebuild_sequences_auto(df, base_stride=5):\n",
    "    trip_len = df.groupby('trip_id').size()\n",
    "    med = int(trip_len.median()) if len(trip_len) else 60\n",
    "    T0  = max(10, min(60, max(10, med // 2)))\n",
    "    stride = max(1, min(base_stride, T0 // 5))\n",
    "\n",
    "    for T_try in [T0, 40, 30, 20, 10]:\n",
    "        seqs = make_sequences(df, seq_len=T_try, stride=stride, sample_hz=CFG['SAMPLE_RATE_HZ'])\n",
    "        if len(seqs['X']) > 0:\n",
    "            CFG['SEQ_LEN'] = T_try\n",
    "            CFG['STRIDE']  = stride\n",
    "            print(f\"[Auto] Using SEQ_LEN={T_try}, STRIDE={stride} ‚Üí windows={len(seqs['X'])}\")\n",
    "            return seqs\n",
    "\n",
    "    print(\"[Auto] ‚ö†Ô∏è Could not create windows with T>=10 ‚Üí fallback to T=1\")\n",
    "    seqs = make_sequences(df, seq_len=1, stride=1, sample_hz=CFG['SAMPLE_RATE_HZ'])\n",
    "    CFG['SEQ_LEN'] = 1\n",
    "    CFG['STRIDE']  = 1\n",
    "    return seqs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cd9f4f",
   "metadata": {},
   "source": [
    "## 6) ‚úÇÔ∏è Split by Driver (No Leakage)\n",
    "We split train/val/test **by driver** so windows from the same driver don‚Äôt leak into other sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8638e7ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T17:12:05.256971Z",
     "iopub.status.busy": "2025-10-08T17:12:05.256621Z",
     "iopub.status.idle": "2025-10-08T17:12:05.276174Z",
     "shell.execute_reply": "2025-10-08T17:12:05.275406Z",
     "shell.execute_reply.started": "2025-10-08T17:12:05.256953Z"
    }
   },
   "outputs": [],
   "source": [
    "# ==== SPLIT with FEATURE METADATA ====\n",
    "def split_dict(d, idx):\n",
    "    out = {\n",
    "        k: (d[k][idx] if isinstance(d[k], np.ndarray) else [d[k][i] for i in idx])\n",
    "        for k in ['X','S','yA','yB','yC','meta']\n",
    "    }\n",
    "    out['dyn_cols'] = d.get('dyn_cols', [])\n",
    "    out['static_cols'] = d.get('static_cols', [])\n",
    "    return out\n",
    "\n",
    "\n",
    "# ==== STACK UTILITY ====\n",
    "def stack_X(X_list):\n",
    "    if len(X_list) == 0:\n",
    "        return np.zeros((0,0,0), dtype=np.float32)\n",
    "    T = len(X_list[0]); F = X_list[0].shape[1] if T > 0 else 0\n",
    "    out = np.zeros((len(X_list), T, F), dtype=np.float32)\n",
    "    for i,x in enumerate(X_list): out[i] = x\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cc974a",
   "metadata": {},
   "source": [
    "## 7) üìè Scaling & PyTorch Datasets\n",
    "We standardize dynamic features using **train only** then wrap them into PyTorch datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58f4e988",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T17:12:05.277511Z",
     "iopub.status.busy": "2025-10-08T17:12:05.276957Z",
     "iopub.status.idle": "2025-10-08T17:12:07.108215Z",
     "shell.execute_reply": "2025-10-08T17:12:07.107451Z",
     "shell.execute_reply.started": "2025-10-08T17:12:05.277486Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Split sizes: 28803 9681 9488\n",
      "‚úÖ Data ready: (28803, 60, 26) (28803, 0) | Dyn: 0 | Static: 0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# ============== GROUPED SPLIT (driver_id or trip_id) ==============\n",
    "drivers = df[['trip_id','driver_id']].drop_duplicates()\n",
    "trip2driver = dict(zip(drivers['trip_id'], drivers['driver_id']))\n",
    "\n",
    "groups_driver = np.array([trip2driver.get(m['trip_id'], -1) for m in data_full['meta']])\n",
    "n_unique_drivers = pd.Series(groups_driver).nunique()\n",
    "\n",
    "if n_unique_drivers < 2:\n",
    "    print(\"[Warn] Only one driver ‚Üí grouping by trip_id instead.\")\n",
    "    groups = np.array([m['trip_id'] for m in data_full['meta']])\n",
    "else:\n",
    "    groups = groups_driver\n",
    "\n",
    "n_samples = len(groups)\n",
    "if n_samples == 0:\n",
    "    raise RuntimeError(\"‚ùå No sequences created. Check your preprocessing.\")\n",
    "\n",
    "if pd.Series(groups).nunique() < 2:\n",
    "    # fallback to random split\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    idx_all = np.arange(n_samples); rng.shuffle(idx_all)\n",
    "    n_test = max(1, int(0.2*n_samples)); n_val = max(1, int(0.1*(n_samples-n_test)))\n",
    "    idx_test = idx_all[:n_test]; idx_val = idx_all[n_test:n_test+n_val]; idx_train = idx_all[n_test+n_val:]\n",
    "else:\n",
    "    gss = GroupShuffleSplit(test_size=0.2, random_state=SEED)\n",
    "    idx_train, idx_test = next(gss.split(np.arange(n_samples), groups=groups))\n",
    "    gss_val = GroupShuffleSplit(test_size=0.125, random_state=SEED)  # ~10% total for val\n",
    "    idx_train, idx_val = next(gss_val.split(idx_train, groups=groups[idx_train]))\n",
    "\n",
    "print(\"‚úÖ Split sizes:\", len(idx_train), len(idx_val), len(idx_test))\n",
    "\n",
    "# ==== APPLY ====\n",
    "if len(data_full['X']) == 0:\n",
    "    data_full = rebuild_sequences_auto(df)\n",
    "\n",
    "train = split_dict(data_full, idx_train)\n",
    "val   = split_dict(data_full, idx_val)\n",
    "test  = split_dict(data_full, idx_test)\n",
    "\n",
    "dyn_cols = train['dyn_cols']\n",
    "static_cols = train['static_cols']\n",
    "\n",
    "# Scale features\n",
    "X_train = stack_X(train['X']); X_val = stack_X(val['X']); X_test = stack_X(test['X'])\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "\n",
    "if X_train.shape[-1] > 0:\n",
    "    scaler.fit(X_train.reshape(-1, X_train.shape[-1]))\n",
    "    def transform(X):\n",
    "        if X.shape[-1]==0: return X\n",
    "        Xr = X.reshape(-1, X.shape[-1]); Xr = scaler.transform(Xr)\n",
    "        return Xr.reshape(X.shape)\n",
    "else:\n",
    "    transform = lambda x: x\n",
    "X_train, X_val, X_test = transform(X_train), transform(X_val), transform(X_test)\n",
    "\n",
    "# Static features\n",
    "S_train = np.array(train['S'], dtype=np.float32) if static_cols else np.zeros((len(train['X']),0), np.float32)\n",
    "S_val   = np.array(val['S'],   dtype=np.float32) if static_cols else np.zeros((len(val['X']),0), np.float32)\n",
    "S_test  = np.array(test['S'],  dtype=np.float32) if static_cols else np.zeros((len(test['X']),0), np.float32)\n",
    "\n",
    "# Labels\n",
    "yA_train, yB_train, yC_train = train['yA'], train['yB'], train['yC']\n",
    "yA_val,   yB_val,   yC_val   = val['yA'],   val['yB'],   val['yC']\n",
    "yA_test,  yB_test,  yC_test  = test['yA'],  test['yB'],  test['yC']\n",
    "\n",
    "# ==== TORCH DATASETS ====\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, S, yA, yB, yC):\n",
    "        self.X, self.S = torch.tensor(X), torch.tensor(S)\n",
    "        self.yA = torch.tensor(yA).float()\n",
    "        self.yB = torch.tensor(yB).float()\n",
    "        self.yC = torch.tensor(yC).float()\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i): return self.X[i], self.S[i], self.yA[i], self.yB[i], self.yC[i]\n",
    "\n",
    "ds_train = SeqDataset(X_train, S_train, yA_train, yB_train, yC_train)\n",
    "ds_val   = SeqDataset(X_val,   S_val,   yA_val,   yB_val,   yC_val)\n",
    "ds_test  = SeqDataset(X_test,  S_test,  yA_test,  yB_test,  yC_test)\n",
    "\n",
    "dl_train = DataLoader(ds_train, batch_size=CFG['BATCH_SIZE'], shuffle=True,  num_workers=4, pin_memory=True)\n",
    "dl_val   = DataLoader(ds_val,   batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=4, pin_memory=True)\n",
    "dl_test  = DataLoader(ds_test,  batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(\"‚úÖ Data ready:\", X_train.shape, S_train.shape, \"| Dyn:\", len(dyn_cols), \"| Static:\", len(static_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4f462b",
   "metadata": {},
   "source": [
    "## 8) üß† Models (Bi-LSTM, TCN-Small, Gating)\n",
    "- <span style=\"color:#6C5CE7\">Gating</span> learns reliability weights per feature (can be disabled in A3).  \n",
    "- <span style=\"color:#E17055\">Bi-LSTM</span> is our main sequence model.  \n",
    "- <span style=\"color:#0984E3\">TCN-Small</span> is an efficient edge variant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7628c998",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T17:12:07.109217Z",
     "iopub.status.busy": "2025-10-08T17:12:07.109008Z",
     "iopub.status.idle": "2025-10-08T17:12:07.128296Z",
     "shell.execute_reply": "2025-10-08T17:12:07.127536Z",
     "shell.execute_reply.started": "2025-10-08T17:12:07.109201Z"
    }
   },
   "outputs": [],
   "source": [
    "# ================== Models ==================\n",
    "class GateLayer(nn.Module):\n",
    "    def __init__(self, in_feats=None):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(in_feats, in_feats) if in_feats else None\n",
    "    def forward(self, x):  # [B,T,F]\n",
    "        F = x.size(-1)\n",
    "        if self.fc is None or self.fc.in_features != F:\n",
    "            self.fc = nn.Linear(F, F).to(x.device)\n",
    "        g = torch.sigmoid(self.fc(x.mean(1)))\n",
    "        return x * g.unsqueeze(1)\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, in_feats, static_feats=0, hidden=128, layers=2, dropout=0.1, gated=True):\n",
    "        super().__init__()\n",
    "        self.gated = gated and in_feats>0\n",
    "        self.gate = GateLayer(in_feats) if self.gated else nn.Identity()\n",
    "        self.lstm = nn.LSTM(input_size=in_feats, hidden_size=hidden, num_layers=layers,\n",
    "                            batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        emb = hidden*2\n",
    "        self.fc_context = nn.Linear(static_feats, hidden) if static_feats>0 else None\n",
    "        head_in = emb + (hidden if self.fc_context else 0)\n",
    "        self.headA, self.headB, self.headC = nn.Linear(head_in,1), nn.Linear(head_in,1), nn.Linear(head_in,1)\n",
    "    def forward(self, x, s):\n",
    "        x = self.gate(x) if self.gated else x\n",
    "        out,_ = self.lstm(x); h = out.mean(1)\n",
    "        if self.fc_context: h = torch.cat([h, torch.relu(self.fc_context(s))], dim=1)\n",
    "        return self.headA(h).squeeze(1), self.headB(h).squeeze(1), self.headC(h).squeeze(1)\n",
    "\n",
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k=3, d=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, k, padding=d*(k-1)//2, dilation=d)\n",
    "        self.bn = nn.BatchNorm1d(out_ch)\n",
    "    def forward(self, x): return torch.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class TCN(nn.Module):\n",
    "    def __init__(self, in_feats, static_feats=0, ch=64, layers=3, gated=True):\n",
    "        super().__init__()\n",
    "        self.gated = gated and in_feats>0\n",
    "        self.gate = GateLayer(in_feats) if self.gated else nn.Identity()\n",
    "        chans = [in_feats, ch, ch, ch][:layers+1]\n",
    "        blocks = [TCNBlock(chans[i], chans[i+1], d=2**i) for i in range(layers)]\n",
    "        self.net = nn.Sequential(*blocks)\n",
    "        self.fc_context = nn.Linear(static_feats, ch) if static_feats>0 else None\n",
    "        head_in = ch + (ch if self.fc_context else 0)\n",
    "        self.headA, self.headB, self.headC = nn.Linear(head_in,1), nn.Linear(head_in,1), nn.Linear(head_in,1)\n",
    "    def forward(self, x, s):\n",
    "        x = self.gate(x) if self.gated else x\n",
    "        z = self.net(x.permute(0,2,1)).mean(-1)\n",
    "        if self.fc_context: z = torch.cat([z, torch.relu(self.fc_context(s))], dim=1)\n",
    "        return self.headA(z).squeeze(1), self.headB(z).squeeze(1), self.headC(z).squeeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71087dd",
   "metadata": {},
   "source": [
    "## 9) üìê Losses & Metrics\n",
    "We compute:\n",
    "- **PR-AUC (A/B)**, **F1 (A)**, **RMSE (C)**, **ECE** (calibration),  \n",
    "- **TTD** (time-to-detection), **FP/h**, **Latency (ms)**, **FLOPs (M)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40cbac90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T17:12:07.129297Z",
     "iopub.status.busy": "2025-10-08T17:12:07.129087Z",
     "iopub.status.idle": "2025-10-08T17:12:07.155917Z",
     "shell.execute_reply": "2025-10-08T17:12:07.155228Z",
     "shell.execute_reply.started": "2025-10-08T17:12:07.129282Z"
    }
   },
   "outputs": [],
   "source": [
    "# ================== Losses / Metrics ==================\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0): super().__init__(); self.alpha,self.gamma=alpha,gamma\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.sigmoid(logits)\n",
    "        ce = F.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        p_t = probs*targets + (1-probs)*(1-targets)\n",
    "        loss = ce * ((1-p_t)**self.gamma) * (self.alpha*targets + (1-self.alpha)*(1-targets))\n",
    "        return loss.mean()\n",
    "\n",
    "def ece_score(y_true, y_prob, n_bins=15):\n",
    "    y_true, y_prob = np.asarray(y_true).astype(int), np.asarray(y_prob)\n",
    "    bins = np.linspace(0.0, 1.0, n_bins+1); inds = np.digitize(y_prob, bins) - 1\n",
    "    ece=0.0\n",
    "    for b in range(n_bins):\n",
    "        mask = inds==b\n",
    "        if not np.any(mask): continue\n",
    "        acc, conf = y_true[mask].mean(), y_prob[mask].mean()\n",
    "        ece += mask.mean() * abs(acc-conf)\n",
    "    return float(ece)\n",
    "\n",
    "def best_f1_threshold(y_true,y_prob):\n",
    "    ps, rs, ts = precision_recall_curve(y_true,y_prob); f1s = 2*ps*rs/(ps+rs+1e-9)\n",
    "    i = np.nanargmax(f1s); return float(ts[i]) if i<len(ts) else 0.5\n",
    "\n",
    "def ttd_and_fprate(meta, y_true, y_prob, threshold=0.5, window_seconds=5):\n",
    "    \"\"\"Compute average time-to-detection (s) and false positives per hour.\"\"\"\n",
    "    y_true = np.asarray(y_true).astype(int)\n",
    "    y_pred = (np.asarray(y_prob) >= threshold).astype(int)\n",
    "\n",
    "    # === FP/h ===\n",
    "    fp = ((y_pred == 1) & (y_true == 0)).sum()\n",
    "    hours = (len(y_true) * window_seconds) / 3600.0\n",
    "    fp_rate = fp / max(hours, 1e-6)\n",
    "\n",
    "    # === TTD (s) ===\n",
    "    ttd_list = []\n",
    "    pos_idx = np.where(y_true == 1)[0]\n",
    "    if len(pos_idx) > 0:\n",
    "        # For each true positive segment, find first detection afterwards\n",
    "        for i in pos_idx:\n",
    "            detect_idx = np.where((y_pred[i:] == 1))[0]\n",
    "            if len(detect_idx) > 0:\n",
    "                delay_s = detect_idx[0] * window_seconds\n",
    "                ttd_list.append(delay_s)\n",
    "        ttd_s = np.mean(ttd_list) if len(ttd_list) > 0 else np.nan\n",
    "    else:\n",
    "        ttd_s = np.nan\n",
    "\n",
    "    return ttd_s, fp_rate\n",
    "\n",
    "import time\n",
    "import torch\n",
    "from thop import profile\n",
    "\n",
    "def measure_latency_flops(model, in_feats, static_feats, seq_len=60, runs=30):\n",
    "    \"\"\"Compute model FLOPs (M) and latency (ms) ‚Äî with GPU or CPU fallback.\"\"\"\n",
    "    model = model.to(device).eval()\n",
    "    X = torch.randn(1, seq_len, in_feats).to(device)\n",
    "    S = torch.randn(1, static_feats).to(device)\n",
    "\n",
    "    # ===== FLOPs estimation =====\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            flops, params = profile(model, inputs=(X, S), verbose=False)\n",
    "        flops_m = flops / 1e6  # millions\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] FLOPs measurement failed: {e}\")\n",
    "        flops_m = np.nan\n",
    "\n",
    "    # ===== Latency measurement =====\n",
    "    timings = []\n",
    "    with torch.no_grad():\n",
    "        # Warm-up runs\n",
    "        for _ in range(5):\n",
    "            _ = model(X, S)\n",
    "\n",
    "        if device.type == \"cuda\":\n",
    "            starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "            for _ in range(runs):\n",
    "                starter.record()\n",
    "                _ = model(X, S)\n",
    "                ender.record()\n",
    "                torch.cuda.synchronize()\n",
    "                timings.append(starter.elapsed_time(ender))  # ms\n",
    "            latency_ms = float(np.mean(timings))\n",
    "        else:\n",
    "            # CPU fallback\n",
    "            for _ in range(runs):\n",
    "                t0 = time.time()\n",
    "                _ = model(X, S)\n",
    "                t1 = time.time()\n",
    "                timings.append((t1 - t0) * 1000)  # convert to ms\n",
    "            latency_ms = float(np.mean(timings))\n",
    "\n",
    "    return flops_m, latency_ms\n",
    "\n",
    "\n",
    "def save_curves(ablation_code, y, p, tag):\n",
    "    # PR\n",
    "    P,R,thr = precision_recall_curve(y,p)\n",
    "    plt.figure(); plt.plot(R,P); plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(f\"{ablation_code} PR {tag}\")\n",
    "    plt.grid(True, alpha=.3); plt.tight_layout(); plt.savefig(PLOT_DIR/f\"{ablation_code}_PR_{tag}.png\"); plt.close()\n",
    "    # ROC\n",
    "    try:\n",
    "        fpr, tpr, _ = roc_curve(y,p)\n",
    "        plt.figure(); plt.plot(fpr,tpr); plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(f\"{ablation_code} ROC {tag}\")\n",
    "        plt.grid(True, alpha=.3); plt.tight_layout(); plt.savefig(PLOT_DIR/f\"{ablation_code}_ROC_{tag}.png\"); plt.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "    # Reliability\n",
    "    bins = np.linspace(0,1,11); inds = np.digitize(p,bins)-1\n",
    "    acc=[]; conf=[]\n",
    "    for b in range(10):\n",
    "        msk=inds==b\n",
    "        if msk.any():\n",
    "            acc.append(y[msk].mean()); conf.append(p[msk].mean())\n",
    "    plt.figure(); plt.plot([0,1],[0,1],'--',alpha=.4); plt.plot(conf, acc, marker='o')\n",
    "    plt.xlabel(\"Confidence\"); plt.ylabel(\"Accuracy\"); plt.title(f\"{ablation_code} Reliability {tag}\")\n",
    "    plt.grid(True, alpha=.3); plt.tight_layout(); plt.savefig(PLOT_DIR/f\"{ablation_code}_Reliability_{tag}.png\"); plt.close()\n",
    "\n",
    "def save_confusion(ablation_code, y, p, tag, thr):\n",
    "    cm = confusion_matrix(y, (p>=thr).astype(int))\n",
    "    plt.figure(); plt.imshow(cm, cmap=\"Blues\"); plt.title(f\"{ablation_code} Confusion {tag}\\n(thr={thr:.3f})\")\n",
    "    plt.colorbar(); plt.xticks([0,1],[\"Pred 0\",\"Pred 1\"]); plt.yticks([0,1],[\"True 0\",\"True 1\"])\n",
    "    for (i,j),v in np.ndenumerate(cm):\n",
    "        plt.text(j,i, int(v), ha=\"center\", va=\"center\")\n",
    "    plt.tight_layout(); plt.savefig(PLOT_DIR/f\"{ablation_code}_Confusion_{tag}.png\"); plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3e2bd1",
   "metadata": {},
   "source": [
    "## 10) üèÉ Training & Evaluation Utilities\n",
    "Includes early stopping, metric computation, latency/FLOPs estimation, and plot saving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74d5055a-ada6-46ea-be50-43f5cd359633",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T17:12:07.156756Z",
     "iopub.status.busy": "2025-10-08T17:12:07.156542Z",
     "iopub.status.idle": "2025-10-08T17:12:07.176812Z",
     "shell.execute_reply": "2025-10-08T17:12:07.176128Z",
     "shell.execute_reply.started": "2025-10-08T17:12:07.156740Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# =================== FIX: use per-split loaders in training ===================\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee981115",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T17:12:07.178361Z",
     "iopub.status.busy": "2025-10-08T17:12:07.177865Z",
     "iopub.status.idle": "2025-10-08T17:12:07.205996Z",
     "shell.execute_reply": "2025-10-08T17:12:07.205213Z",
     "shell.execute_reply.started": "2025-10-08T17:12:07.178341Z"
    }
   },
   "outputs": [],
   "source": [
    "# ================== Training Loop ==================\n",
    "def make_loaders(ds_tr, ds_va, batch):\n",
    "    return (DataLoader(ds_tr,batch,True,4,pin_memory=True),\n",
    "            DataLoader(ds_va,batch,False,4,pin_memory=True))\n",
    "\n",
    "def train_model(model, ablation_name, loss_mode, dl_tr, dl_va, yA_for_weights, yB_for_weights):\n",
    "    model = model.to(device); opt = AdamW(model.parameters(), lr=CFG['LR'])\n",
    "    scaler = GradScaler(enabled=(device.type==\"cuda\"))\n",
    "    cwA = compute_class_weight('balanced', classes=np.array([0,1]), y=yA_for_weights).tolist()\n",
    "    cwB = compute_class_weight('balanced', classes=np.array([0,1]), y=yB_for_weights).tolist()\n",
    "    posA,posB = torch.tensor(cwA[1]/cwA[0],device=device),torch.tensor(cwB[1]/cwB[0],device=device)\n",
    "    focal = FocalLoss().to(device)\n",
    "    best_val=float(\"inf\"); bad=0\n",
    "\n",
    "    for ep in range(1, CFG['EPOCHS']+1):\n",
    "        model.train(); tr_loss=0.0; opt.zero_grad()\n",
    "        for i,(X,S,ya,yb,yc) in enumerate(dl_tr):\n",
    "            X,S,ya,yb,yc = X.to(device),S.to(device),ya.to(device),yb.to(device),yc.to(device)\n",
    "            with autocast(enabled=(device.type==\"cuda\")):\n",
    "                a,b,c = model(X,S)\n",
    "                la = focal(a,ya) if loss_mode=='focal' else F.binary_cross_entropy_with_logits(a,ya,pos_weight=posA)\n",
    "                lb = focal(b,yb) if loss_mode=='focal' else F.binary_cross_entropy_with_logits(b,yb,pos_weight=posB)\n",
    "                lc = F.smooth_l1_loss(c,yc)\n",
    "                loss=(la+lb+0.5*lc)/2\n",
    "            scaler.scale(loss).backward(); scaler.step(opt); scaler.update(); opt.zero_grad()\n",
    "            tr_loss += loss.item()\n",
    "        # Validation\n",
    "        model.eval(); vl=0.0;n=0;yA,yB,pa,pb=[],[],[],[]\n",
    "        with torch.no_grad(), autocast(enabled=(device.type==\"cuda\")):\n",
    "            for X,S,ya,yb,yc in dl_va:\n",
    "                X,S,ya,yb,yc = X.to(device),S.to(device),ya.to(device),yb.to(device),yc.to(device)\n",
    "                a,b,c = model(X,S)\n",
    "                la=F.binary_cross_entropy_with_logits(a,ya); lb=F.binary_cross_entropy_with_logits(b,yb); lc=F.smooth_l1_loss(c,yc)\n",
    "                vl+=(la+lb+0.5*lc).item(); n+=1\n",
    "                yA.extend(ya.cpu().numpy()); pa.extend(torch.sigmoid(a).cpu().numpy())\n",
    "                yB.extend(yb.cpu().numpy()); pb.extend(torch.sigmoid(b).cpu().numpy())\n",
    "        vl/=max(1,n)\n",
    "        prA, f1A = average_precision_score(yA,pa), f1_score(yA,(np.array(pa)>=0.5).astype(int))\n",
    "        prB, f1B = average_precision_score(yB,pb), f1_score(yB,(np.array(pb)>=0.5).astype(int))\n",
    "        print(f\"[{ablation_name}] epoch {ep} train_loss={tr_loss/len(dl_tr):.4f} val_loss={vl:.4f} \"\n",
    "              f\"A: PR-AUC={prA:.3f} F1={f1A:.3f}  B: PR-AUC={prB:.3f} F1={f1B:.3f}\")\n",
    "        if vl<best_val: best_val=vl; bad=0; torch.save(model.state_dict(), MODEL_DIR/f\"{ablation_name}_best.pt\")\n",
    "        else: bad+=1; \n",
    "        if bad>=CFG['PATIENCE']: print(f\"üõë Early stopping at epoch {ep}\"); break\n",
    "    model.load_state_dict(torch.load(MODEL_DIR/f\"{ablation_name}_best.pt\")); return model.eval()\n",
    "\n",
    "# ================== Eval ==================\n",
    "def predict_model(model, ds, batch=CFG['BATCH_SIZE']):\n",
    "    dl = DataLoader(ds, batch_size=batch, shuffle=False, num_workers=4, pin_memory=True)\n",
    "    pa, pb, pc = [], [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad(), autocast(enabled=(device.type==\"cuda\")):\n",
    "        for X, S, _, _, _ in dl:\n",
    "            X = X.to(device); S = S.to(device)\n",
    "            a, b, c = model(X, S)\n",
    "            pa.extend(torch.sigmoid(a).cpu().numpy())\n",
    "            pb.extend(torch.sigmoid(b).cpu().numpy())\n",
    "            pc.extend(c.cpu().numpy())\n",
    "    return np.array(pa), np.array(pb), np.array(pc)\n",
    "\n",
    "def eval_all(name, yA, yB, yC, pA, pB, pC, meta):\n",
    "    thA, thB = best_f1_threshold(yA, pA), best_f1_threshold(yB, pB)\n",
    "    ttd_s, fp_h = ttd_and_fprate(meta, yA, pA, threshold=thA)\n",
    "    return dict(\n",
    "        precision_auc_A=average_precision_score(yA, pA),\n",
    "        f1_A=f1_score(yA, (pA >= thA).astype(int)),\n",
    "        precision_auc_B=average_precision_score(yB, pB),\n",
    "        rmse_C=math.sqrt(mean_squared_error(yC, pC)),\n",
    "        ece=ece_score(yA, pA),\n",
    "        ttd_s=ttd_s,\n",
    "        fp_per_hour=fp_h\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccf5a68",
   "metadata": {},
   "source": [
    "## 11) üå≥ A1 ‚Äî <span style=\"color:#6C5CE7\">GBM (No Mobility, Temporal-Agnostic)</span>\n",
    "Replace sequence DL with **static aggregates** (mean/std/min/max over window).  \n",
    "**Hypothesis:** PR-AUC drops; **TTD** increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d230f27b-f454-4aec-8050-668552db66f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T17:12:07.207062Z",
     "iopub.status.busy": "2025-10-08T17:12:07.206848Z",
     "iopub.status.idle": "2025-10-08T17:12:07.228121Z",
     "shell.execute_reply": "2025-10-08T17:12:07.227535Z",
     "shell.execute_reply.started": "2025-10-08T17:12:07.207038Z"
    }
   },
   "outputs": [],
   "source": [
    "# ================== GBM Baseline (A1) ==================\n",
    "def aggregate_for_gbm(split):\n",
    "    feats=[np.concatenate([X.mean(0),X.std(0),X.min(0),X.max(0)]) if X.shape[-1]>0 else np.zeros(4) for X in split['X']]\n",
    "    return np.array(feats,np.float32), np.array(split['yA']), np.array(split['yB']), np.array(split['yC'])\n",
    "\n",
    "def run_gbm(name=\"A1_GBM\"):\n",
    "    Xtr,yAtr,yBtr,yCtr = aggregate_for_gbm(train)\n",
    "    Xva,yAva,yBva,yCva = aggregate_for_gbm(val)\n",
    "    Xte,yAte,yBte,yCte = aggregate_for_gbm(test)\n",
    "\n",
    "    gbmA = lgb.LGBMClassifier(\n",
    "        n_estimators=1000, learning_rate=0.05, num_leaves=64,\n",
    "        objective=\"binary\", random_state=SEED\n",
    "    )\n",
    "    gbmB = lgb.LGBMClassifier(\n",
    "        n_estimators=1000, learning_rate=0.05, num_leaves=64,\n",
    "        objective=\"binary\", random_state=SEED\n",
    "    )\n",
    "    gbrC = lgb.LGBMRegressor(\n",
    "        n_estimators=1000, learning_rate=0.05, num_leaves=64,\n",
    "        objective=\"regression\", random_state=SEED\n",
    "    )\n",
    "\n",
    "    # Fit with early stopping\n",
    "    gbmA.fit(Xtr, yAtr,\n",
    "             eval_set=[(Xva, yAva)],\n",
    "             eval_metric=\"average_precision\",\n",
    "             callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(0)])  # 0 = silent\n",
    "    gbmB.fit(Xtr, yBtr,\n",
    "             eval_set=[(Xva, yBva)],\n",
    "             eval_metric=\"average_precision\",\n",
    "             callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(0)])\n",
    "    gbrC.fit(Xtr, yCtr,\n",
    "             eval_set=[(Xva, yCva)],\n",
    "             eval_metric=\"rmse\",\n",
    "             callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(0)])\n",
    "\n",
    "    # Predictions\n",
    "    pA = gbmA.predict_proba(Xte)[:,1]\n",
    "    pB = gbmB.predict_proba(Xte)[:,1]\n",
    "    pC = gbrC.predict(Xte)\n",
    "\n",
    "    return eval_all(\"A1\", yAte, yBte, yCte, pA, pB, pC, test['meta'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb563a8",
   "metadata": {},
   "source": [
    "## 12) üß™ Ablations A2‚ÄìA10 (one-by-one)\n",
    "We implement each hypothesis exactly as described and log metrics & plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "860643f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T17:12:07.228987Z",
     "iopub.status.busy": "2025-10-08T17:12:07.228792Z",
     "iopub.status.idle": "2025-10-08T17:12:07.249976Z",
     "shell.execute_reply": "2025-10-08T17:12:07.249276Z",
     "shell.execute_reply.started": "2025-10-08T17:12:07.228972Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_ablation(code):\n",
    "    # Defaults\n",
    "    ablate=None; short_hist=False; gated=True; kind=\"lstm\"; big=False; small=False; loss_mode='bce_weighted'\n",
    "    if code==\"A2\": ablate=\"A2-nocontext\"\n",
    "    if code==\"A3\": gated=False\n",
    "    if code==\"A4\": big=True\n",
    "    if code==\"A5\": ablate=\"A5-jitter\"\n",
    "    if code==\"A6\": short_hist=True\n",
    "    if code==\"A7\": gated=True\n",
    "    if code==\"A8\": ablate=\"A8-mapfree\"\n",
    "    if code==\"A9\": loss_mode='focal'\n",
    "    if code==\"A10\": kind=\"tcn\"; small=True\n",
    "\n",
    "    # Build per-ablation datasets (if features/horizon change)\n",
    "    if code in [\"A2\",\"A5\",\"A6\",\"A8\"]:\n",
    "        seqs = make_sequences(df, seq_len=CFG['SEQ_LEN'], stride=CFG['STRIDE'],\n",
    "                              sample_hz=CFG['SAMPLE_RATE_HZ'], ablation=ablate,\n",
    "                              short_history=short_hist)\n",
    "\n",
    "        def re_split(seqs, idx):\n",
    "            get = lambda k: [seqs[k][i] for i in idx]\n",
    "            return dict(X=get('X'), S=get('S'),\n",
    "                        yA=np.array([seqs['yA'][i] for i in idx]),\n",
    "                        yB=np.array([seqs['yB'][i] for i in idx]),\n",
    "                        yC=np.array([seqs['yC'][i] for i in idx]),\n",
    "                        meta=[seqs['meta'][i] for i in idx])\n",
    "\n",
    "        tr, va, te = re_split(seqs, idx_train), re_split(seqs, idx_val), re_split(seqs, idx_test)\n",
    "\n",
    "        # stack + scale\n",
    "        def stack(XL):\n",
    "            T = len(XL[0]); F = XL[0].shape[1] if T>0 else 0\n",
    "            out = np.zeros((len(XL), T, F), dtype=np.float32)\n",
    "            for i,x in enumerate(XL): out[i]=x\n",
    "            return out\n",
    "\n",
    "        Xtr, Xva, Xte = stack(tr['X']), stack(va['X']), stack(te['X'])\n",
    "        in_feats = Xtr.shape[-1]\n",
    "        static_feats = (len(tr['S'][0]) if len(tr['S'])>0 and hasattr(tr['S'][0],'__len__') else 0)\n",
    "\n",
    "        scaler2 = StandardScaler()\n",
    "        if in_feats>0:\n",
    "            scaler2.fit(Xtr.reshape(-1, in_feats))\n",
    "            def T(x):\n",
    "                if x.shape[-1]==0: return x\n",
    "                xr = x.reshape(-1, in_feats); xr = scaler2.transform(xr)\n",
    "                return xr.reshape(x.shape)\n",
    "        else:\n",
    "            T = lambda x: x\n",
    "\n",
    "        Xtr, Xva, Xte = T(Xtr), T(Xva), T(Xte)\n",
    "        Str, Sva, Ste = np.array(tr['S'], np.float32), np.array(va['S'], np.float32), np.array(te['S'], np.float32)\n",
    "\n",
    "        ds_tr = SeqDataset(Xtr, Str, tr['yA'], tr['yB'], tr['yC'])\n",
    "        ds_va = SeqDataset(Xva, Sva, va['yA'], va['yB'], va['yC'])\n",
    "        ds_te = SeqDataset(Xte, Ste, te['yA'], te['yB'], te['yC'])\n",
    "        yA_w, yB_w = tr['yA'], tr['yB']\n",
    "\n",
    "        dl_tr, dl_va = make_loaders(ds_tr, ds_va, CFG['BATCH_SIZE'])\n",
    "    else:\n",
    "        # reuse global prebuilt datasets/loaders\n",
    "        ds_tr, ds_va, ds_te = ds_train, ds_val, ds_test\n",
    "        dl_tr, dl_va = dl_train, dl_val\n",
    "        in_feats, static_feats = X_train.shape[-1], S_train.shape[-1]\n",
    "        yA_w, yB_w = yA_train, yB_train\n",
    "\n",
    "    # build & train on the loaders that match this ablation's feature dims\n",
    "    mdl = make_model(kind, in_feats, static_feats, big=big, small=small, gated=gated)\n",
    "    mdl = train_model(mdl, code, loss_mode, dl_tr, dl_va, yA_w, yB_w)\n",
    "\n",
    "    # predict + eval on the matching test set\n",
    "    pA, pB, pC = predict_model(mdl, ds_te, batch=CFG['BATCH_SIZE'])\n",
    "    metrics = eval_all(code, ds_te.yA.numpy(), ds_te.yB.numpy(), ds_te.yC.numpy(), pA, pB, pC, (test['meta'] if code not in [\"A2\",\"A5\",\"A6\",\"A8\"] else te['meta']))\n",
    "    flops_m, ms = measure_latency_flops(mdl, in_feats, static_feats, seq_len=CFG['SEQ_LEN'])\n",
    "    metrics['latency_ms'] = ms; metrics['flops_m'] = flops_m\n",
    "\n",
    "    if code==\"A7\":\n",
    "        Xte = ds_te.X.clone()\n",
    "        sensor_names = ['speed','acceleration','rpm','steering_angle']\n",
    "        dyn_cols_here = data_full.get('dyn_cols', [])\n",
    "        sensor_idx = [i for i,c in enumerate(dyn_cols_here) if c in sensor_names]\n",
    "    \n",
    "        rng = np.random.default_rng(SEED)\n",
    "        for i in range(Xte.shape[0]):\n",
    "            for j in sensor_idx:\n",
    "                if rng.uniform() < 0.2:\n",
    "                    miss_idx = rng.integers(0, Xte.shape[1], size=max(1, Xte.shape[1]//10))\n",
    "                    Xte[i, miss_idx, j] = 0.0\n",
    "    \n",
    "        ds_drop = SeqDataset(Xte.numpy(), ds_te.S.numpy(), ds_te.yA.numpy(), ds_te.yB.numpy(), ds_te.yC.numpy())\n",
    "        pA2, pB2, pC2 = predict_model(mdl, ds_drop, batch=CFG['BATCH_SIZE'])\n",
    "        m2 = eval_all(code+\"-dropout\", ds_drop.yA.numpy(), ds_drop.yB.numpy(), ds_drop.yC.numpy(),\n",
    "                      pA2, pB2, pC2, (test['meta'] if code not in [\"A2\",\"A5\",\"A6\",\"A8\"] else te['meta']))\n",
    "        metrics['robust_dPR_A'] = m2['precision_auc_A'] - metrics['precision_auc_A']\n",
    "        metrics['robust_dFPH']  = m2['fp_per_hour'] - metrics['fp_per_hour']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4acd218d-ef15-4dc0-a944-7da36308b380",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T17:12:07.250824Z",
     "iopub.status.busy": "2025-10-08T17:12:07.250610Z",
     "iopub.status.idle": "2025-10-08T17:14:38.570071Z",
     "shell.execute_reply": "2025-10-08T17:14:38.569156Z",
     "shell.execute_reply.started": "2025-10-08T17:12:07.250800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BiLSTM_Full] epoch 1 train_loss=0.4868 val_loss=1.2239 A: PR-AUC=0.804 F1=0.659  B: PR-AUC=0.820 F1=0.650\n",
      "[BiLSTM_Full] epoch 2 train_loss=0.4268 val_loss=1.2232 A: PR-AUC=0.830 F1=0.616  B: PR-AUC=0.844 F1=0.629\n",
      "[BiLSTM_Full] epoch 3 train_loss=0.4193 val_loss=1.1782 A: PR-AUC=0.833 F1=0.669  B: PR-AUC=0.847 F1=0.670\n",
      "[BiLSTM_Full] epoch 5 train_loss=0.4099 val_loss=1.1836 A: PR-AUC=0.837 F1=0.672  B: PR-AUC=0.852 F1=0.670\n",
      "[BiLSTM_Full] epoch 6 train_loss=0.4043 val_loss=1.1865 A: PR-AUC=0.843 F1=0.668  B: PR-AUC=0.856 F1=0.679\n",
      "[BiLSTM_Full] epoch 7 train_loss=0.3968 val_loss=1.1971 A: PR-AUC=0.845 F1=0.667  B: PR-AUC=0.861 F1=0.673\n",
      "[BiLSTM_Full] epoch 8 train_loss=0.3893 val_loss=1.1114 A: PR-AUC=0.854 F1=0.734  B: PR-AUC=0.870 F1=0.744\n",
      "[BiLSTM_Full] epoch 9 train_loss=0.3737 val_loss=1.1094 A: PR-AUC=0.859 F1=0.739  B: PR-AUC=0.873 F1=0.749\n",
      "[BiLSTM_Full] epoch 10 train_loss=0.3576 val_loss=1.1670 A: PR-AUC=0.861 F1=0.707  B: PR-AUC=0.879 F1=0.730\n",
      "[BiLSTM_Full] epoch 11 train_loss=0.3385 val_loss=1.0656 A: PR-AUC=0.872 F1=0.784  B: PR-AUC=0.885 F1=0.774\n",
      "[BiLSTM_Full] epoch 12 train_loss=0.3110 val_loss=1.0582 A: PR-AUC=0.882 F1=0.765  B: PR-AUC=0.898 F1=0.790\n",
      "[BiLSTM_Full] epoch 13 train_loss=0.2819 val_loss=1.0312 A: PR-AUC=0.889 F1=0.787  B: PR-AUC=0.905 F1=0.805\n",
      "[BiLSTM_Full] epoch 14 train_loss=0.2507 val_loss=1.0141 A: PR-AUC=0.895 F1=0.803  B: PR-AUC=0.908 F1=0.818\n",
      "[BiLSTM_Full] epoch 15 train_loss=0.2243 val_loss=0.9629 A: PR-AUC=0.899 F1=0.824  B: PR-AUC=0.917 F1=0.832\n",
      "[BiLSTM_Full] epoch 16 train_loss=0.1914 val_loss=1.0628 A: PR-AUC=0.902 F1=0.814  B: PR-AUC=0.920 F1=0.815\n",
      "[BiLSTM_Full] epoch 17 train_loss=0.1638 val_loss=0.9685 A: PR-AUC=0.911 F1=0.843  B: PR-AUC=0.920 F1=0.847\n",
      "[BiLSTM_Full] epoch 18 train_loss=0.1427 val_loss=0.9845 A: PR-AUC=0.910 F1=0.856  B: PR-AUC=0.921 F1=0.853\n",
      "[BiLSTM_Full] epoch 19 train_loss=0.1201 val_loss=0.9936 A: PR-AUC=0.916 F1=0.859  B: PR-AUC=0.927 F1=0.859\n",
      "[BiLSTM_Full] epoch 20 train_loss=0.1026 val_loss=1.0419 A: PR-AUC=0.916 F1=0.839  B: PR-AUC=0.931 F1=0.862\n",
      "üõë Early stopping at epoch 20\n",
      "Full BiLSTM: {'precision_auc_A': 0.9474697019587588, 'f1_A': 0.900969774191265, 'precision_auc_B': 0.9445388310414332, 'rmse_C': 0.06403554599628089, 'ece': 0.05004494266606504, 'ttd_s': 0.4528219337902873, 'fp_per_hour': 68.75210792580101}\n"
     ]
    }
   ],
   "source": [
    "# ================== Baseline BiLSTM Full ==================\n",
    "# Train\n",
    "full = BiLSTM(X_train.shape[-1], S_train.shape[-1], hidden=CFG['HIDDEN'], dropout=CFG['DROPOUT'], gated=True)\n",
    "full = train_model(full, \"BiLSTM_Full\", \"bce_weighted\", dl_train, dl_val, yA_train, yB_train)\n",
    "\n",
    "# Predict + Eval\n",
    "ds_test = SeqDataset(X_test, S_test, yA_test, yB_test, yC_test)   # <-- ensure this\n",
    "pA, pB, pC = predict_model(full, ds_test)\n",
    "met_full = eval_all(\"Full\", yA_test, yB_test, yC_test, pA, pB, pC, test['meta'])\n",
    "print(\"Full BiLSTM:\", met_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "028b31b5-6483-424b-a0e0-4d3e72218206",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T17:14:38.572181Z",
     "iopub.status.busy": "2025-10-08T17:14:38.571419Z",
     "iopub.status.idle": "2025-10-08T17:15:20.976585Z",
     "shell.execute_reply": "2025-10-08T17:15:20.975759Z",
     "shell.execute_reply.started": "2025-10-08T17:14:38.572153Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 18464, number of negative: 10339\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017901 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16385\n",
      "[LightGBM] [Info] Number of data points in the train set: 28803, number of used features: 101\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.641044 -> initscore=0.579900\n",
      "[LightGBM] [Info] Start training from score 0.579900\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's average_precision: 0.948609\tvalid_0's binary_logloss: 0.340291\n",
      "[LightGBM] [Info] Number of positive: 18327, number of negative: 10476\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.016577 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16385\n",
      "[LightGBM] [Info] Number of data points in the train set: 28803, number of used features: 101\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.636288 -> initscore=0.559288\n",
      "[LightGBM] [Info] Start training from score 0.559288\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\tvalid_0's average_precision: 0.954704\tvalid_0's binary_logloss: 0.325119\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.017038 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16385\n",
      "[LightGBM] [Info] Number of data points in the train set: 28803, number of used features: 101\n",
      "[LightGBM] [Info] Start training from score 0.284954\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's rmse: 0.0510223\tvalid_0's l2: 0.00260327\n",
      "GBM: {'precision_auc_A': 0.9825485940431886, 'f1_A': 0.958790224324518, 'precision_auc_B': 0.985422939676381, 'rmse_C': 0.03329294137313945, 'ece': 0.06404407569862597, 'ttd_s': 0.16114919060813768, 'fp_per_hour': 30.88532883642496}\n"
     ]
    }
   ],
   "source": [
    "# === Run GBM baseline (A1) ===\n",
    "res_A1 = run_gbm()\n",
    "print(\"GBM:\", res_A1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8337d12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T17:15:20.977688Z",
     "iopub.status.busy": "2025-10-08T17:15:20.977466Z",
     "iopub.status.idle": "2025-10-08T17:47:42.462978Z",
     "shell.execute_reply": "2025-10-08T17:47:42.462246Z",
     "shell.execute_reply.started": "2025-10-08T17:15:20.977672Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A2] epoch 1 train_loss=0.4860 val_loss=1.2171 A: PR-AUC=0.821 F1=0.647  B: PR-AUC=0.831 F1=0.651\n",
      "[A2] epoch 2 train_loss=0.4273 val_loss=1.1863 A: PR-AUC=0.833 F1=0.661  B: PR-AUC=0.846 F1=0.653\n",
      "[A2] epoch 3 train_loss=0.4181 val_loss=1.2092 A: PR-AUC=0.835 F1=0.646  B: PR-AUC=0.846 F1=0.644\n",
      "[A2] epoch 4 train_loss=0.4138 val_loss=1.1917 A: PR-AUC=0.837 F1=0.647  B: PR-AUC=0.851 F1=0.667\n",
      "[A2] epoch 5 train_loss=0.4096 val_loss=1.1497 A: PR-AUC=0.840 F1=0.699  B: PR-AUC=0.853 F1=0.712\n",
      "[A2] epoch 6 train_loss=0.4043 val_loss=1.1982 A: PR-AUC=0.841 F1=0.652  B: PR-AUC=0.856 F1=0.668\n",
      "[A2] epoch 7 train_loss=0.3966 val_loss=1.1716 A: PR-AUC=0.847 F1=0.687  B: PR-AUC=0.863 F1=0.687\n",
      "[A2] epoch 8 train_loss=0.3865 val_loss=1.1563 A: PR-AUC=0.851 F1=0.729  B: PR-AUC=0.867 F1=0.688\n",
      "[A2] epoch 9 train_loss=0.3723 val_loss=1.1193 A: PR-AUC=0.853 F1=0.736  B: PR-AUC=0.874 F1=0.734\n",
      "[A2] epoch 10 train_loss=0.3562 val_loss=1.1185 A: PR-AUC=0.865 F1=0.752  B: PR-AUC=0.883 F1=0.738\n",
      "[A2] epoch 11 train_loss=0.3349 val_loss=1.0666 A: PR-AUC=0.869 F1=0.764  B: PR-AUC=0.892 F1=0.778\n",
      "[A2] epoch 12 train_loss=0.3085 val_loss=1.0399 A: PR-AUC=0.881 F1=0.789  B: PR-AUC=0.898 F1=0.786\n",
      "[A2] epoch 13 train_loss=0.2775 val_loss=0.9966 A: PR-AUC=0.886 F1=0.795  B: PR-AUC=0.896 F1=0.820\n",
      "[A2] epoch 14 train_loss=0.2475 val_loss=1.0088 A: PR-AUC=0.893 F1=0.803  B: PR-AUC=0.907 F1=0.814\n",
      "[A2] epoch 15 train_loss=0.2183 val_loss=0.9786 A: PR-AUC=0.903 F1=0.815  B: PR-AUC=0.911 F1=0.837\n",
      "[A2] epoch 16 train_loss=0.1931 val_loss=0.9731 A: PR-AUC=0.905 F1=0.831  B: PR-AUC=0.912 F1=0.838\n",
      "[A2] epoch 17 train_loss=0.1676 val_loss=0.9707 A: PR-AUC=0.907 F1=0.840  B: PR-AUC=0.918 F1=0.852\n",
      "[A2] epoch 18 train_loss=0.1393 val_loss=1.0370 A: PR-AUC=0.901 F1=0.833  B: PR-AUC=0.921 F1=0.842\n",
      "[A2] epoch 19 train_loss=0.1175 val_loss=1.0364 A: PR-AUC=0.913 F1=0.861  B: PR-AUC=0.922 F1=0.851\n",
      "[A2] epoch 20 train_loss=0.1033 val_loss=1.0303 A: PR-AUC=0.918 F1=0.860  B: PR-AUC=0.923 F1=0.855\n",
      "[A2] epoch 21 train_loss=0.0893 val_loss=1.0461 A: PR-AUC=0.913 F1=0.867  B: PR-AUC=0.923 F1=0.870\n",
      "[A2] epoch 22 train_loss=0.0750 val_loss=1.0988 A: PR-AUC=0.911 F1=0.864  B: PR-AUC=0.924 F1=0.870\n",
      "üõë Early stopping at epoch 22\n",
      "[A3] epoch 1 train_loss=0.4846 val_loss=1.2635 A: PR-AUC=0.816 F1=0.644  B: PR-AUC=0.829 F1=0.628\n",
      "[A3] epoch 2 train_loss=0.4274 val_loss=1.1695 A: PR-AUC=0.829 F1=0.689  B: PR-AUC=0.843 F1=0.683\n",
      "[A3] epoch 4 train_loss=0.4156 val_loss=1.1990 A: PR-AUC=0.834 F1=0.645  B: PR-AUC=0.850 F1=0.644\n",
      "[A3] epoch 5 train_loss=0.4110 val_loss=1.1992 A: PR-AUC=0.839 F1=0.636  B: PR-AUC=0.853 F1=0.646\n",
      "[A3] epoch 6 train_loss=0.4055 val_loss=1.2001 A: PR-AUC=0.841 F1=0.658  B: PR-AUC=0.857 F1=0.658\n",
      "[A3] epoch 7 train_loss=0.4038 val_loss=1.1850 A: PR-AUC=0.843 F1=0.675  B: PR-AUC=0.858 F1=0.669\n",
      "üõë Early stopping at epoch 7\n",
      "[A5] epoch 1 train_loss=0.4887 val_loss=1.2517 A: PR-AUC=0.804 F1=0.664  B: PR-AUC=0.813 F1=0.657\n",
      "[A5] epoch 2 train_loss=0.4301 val_loss=1.2444 A: PR-AUC=0.831 F1=0.602  B: PR-AUC=0.844 F1=0.614\n",
      "[A5] epoch 3 train_loss=0.4185 val_loss=1.2128 A: PR-AUC=0.833 F1=0.630  B: PR-AUC=0.847 F1=0.642\n",
      "[A5] epoch 4 train_loss=0.4149 val_loss=1.1844 A: PR-AUC=0.838 F1=0.661  B: PR-AUC=0.849 F1=0.658\n",
      "[A5] epoch 5 train_loss=0.4081 val_loss=1.1742 A: PR-AUC=0.835 F1=0.666  B: PR-AUC=0.852 F1=0.716\n",
      "[A5] epoch 6 train_loss=0.4035 val_loss=1.1717 A: PR-AUC=0.846 F1=0.684  B: PR-AUC=0.857 F1=0.695\n",
      "[A5] epoch 7 train_loss=0.3956 val_loss=1.1924 A: PR-AUC=0.843 F1=0.656  B: PR-AUC=0.862 F1=0.671\n",
      "[A5] epoch 8 train_loss=0.3864 val_loss=1.1366 A: PR-AUC=0.853 F1=0.706  B: PR-AUC=0.868 F1=0.728\n",
      "[A5] epoch 9 train_loss=0.3718 val_loss=1.1478 A: PR-AUC=0.860 F1=0.705  B: PR-AUC=0.873 F1=0.722\n",
      "[A5] epoch 10 train_loss=0.3549 val_loss=1.0772 A: PR-AUC=0.866 F1=0.756  B: PR-AUC=0.879 F1=0.774\n",
      "[A5] epoch 11 train_loss=0.3342 val_loss=1.0710 A: PR-AUC=0.875 F1=0.752  B: PR-AUC=0.890 F1=0.777\n",
      "[A5] epoch 12 train_loss=0.3069 val_loss=1.0328 A: PR-AUC=0.883 F1=0.791  B: PR-AUC=0.898 F1=0.786\n",
      "[A5] epoch 13 train_loss=0.2791 val_loss=0.9673 A: PR-AUC=0.893 F1=0.831  B: PR-AUC=0.904 F1=0.817\n",
      "[A5] epoch 14 train_loss=0.2449 val_loss=0.9710 A: PR-AUC=0.897 F1=0.812  B: PR-AUC=0.909 F1=0.836\n",
      "[A5] epoch 15 train_loss=0.2135 val_loss=0.9676 A: PR-AUC=0.902 F1=0.832  B: PR-AUC=0.915 F1=0.828\n",
      "[A5] epoch 16 train_loss=0.1872 val_loss=0.9533 A: PR-AUC=0.908 F1=0.823  B: PR-AUC=0.917 F1=0.852\n",
      "[A5] epoch 17 train_loss=0.1563 val_loss=0.9635 A: PR-AUC=0.913 F1=0.853  B: PR-AUC=0.924 F1=0.843\n",
      "[A5] epoch 18 train_loss=0.1362 val_loss=0.9847 A: PR-AUC=0.905 F1=0.850  B: PR-AUC=0.921 F1=0.863\n",
      "[A5] epoch 19 train_loss=0.1156 val_loss=1.0188 A: PR-AUC=0.917 F1=0.854  B: PR-AUC=0.923 F1=0.858\n",
      "[A5] epoch 20 train_loss=0.0977 val_loss=1.0231 A: PR-AUC=0.916 F1=0.863  B: PR-AUC=0.925 F1=0.860\n",
      "[A5] epoch 21 train_loss=0.0869 val_loss=1.0385 A: PR-AUC=0.915 F1=0.863  B: PR-AUC=0.928 F1=0.868\n",
      "üõë Early stopping at epoch 21\n",
      "[A6] epoch 1 train_loss=0.4831 val_loss=1.2735 A: PR-AUC=0.819 F1=0.577  B: PR-AUC=0.830 F1=0.563\n",
      "[A6] epoch 2 train_loss=0.4251 val_loss=1.2561 A: PR-AUC=0.829 F1=0.610  B: PR-AUC=0.844 F1=0.601\n",
      "[A6] epoch 3 train_loss=0.4185 val_loss=1.1627 A: PR-AUC=0.834 F1=0.686  B: PR-AUC=0.847 F1=0.694\n",
      "[A6] epoch 4 train_loss=0.4139 val_loss=1.1764 A: PR-AUC=0.838 F1=0.663  B: PR-AUC=0.851 F1=0.675\n",
      "[A6] epoch 5 train_loss=0.4092 val_loss=1.1708 A: PR-AUC=0.838 F1=0.674  B: PR-AUC=0.853 F1=0.691\n",
      "[A6] epoch 6 train_loss=0.4035 val_loss=1.2065 A: PR-AUC=0.845 F1=0.653  B: PR-AUC=0.857 F1=0.669\n",
      "[A6] epoch 7 train_loss=0.3951 val_loss=1.1304 A: PR-AUC=0.848 F1=0.708  B: PR-AUC=0.863 F1=0.739\n",
      "[A6] epoch 8 train_loss=0.3855 val_loss=1.1439 A: PR-AUC=0.848 F1=0.716  B: PR-AUC=0.869 F1=0.707\n",
      "[A6] epoch 9 train_loss=0.3706 val_loss=1.1099 A: PR-AUC=0.859 F1=0.740  B: PR-AUC=0.875 F1=0.744\n",
      "[A6] epoch 10 train_loss=0.3528 val_loss=1.0861 A: PR-AUC=0.861 F1=0.760  B: PR-AUC=0.883 F1=0.767\n",
      "[A6] epoch 11 train_loss=0.3321 val_loss=1.1288 A: PR-AUC=0.866 F1=0.735  B: PR-AUC=0.890 F1=0.738\n",
      "[A6] epoch 12 train_loss=0.3095 val_loss=1.0535 A: PR-AUC=0.879 F1=0.774  B: PR-AUC=0.896 F1=0.791\n",
      "[A6] epoch 13 train_loss=0.2798 val_loss=1.0143 A: PR-AUC=0.882 F1=0.802  B: PR-AUC=0.898 F1=0.808\n",
      "[A6] epoch 14 train_loss=0.2524 val_loss=1.0101 A: PR-AUC=0.890 F1=0.809  B: PR-AUC=0.908 F1=0.811\n",
      "[A6] epoch 15 train_loss=0.2243 val_loss=0.9858 A: PR-AUC=0.898 F1=0.821  B: PR-AUC=0.911 F1=0.827\n",
      "[A6] epoch 16 train_loss=0.1956 val_loss=0.9831 A: PR-AUC=0.902 F1=0.822  B: PR-AUC=0.913 F1=0.839\n",
      "[A6] epoch 17 train_loss=0.1764 val_loss=1.0171 A: PR-AUC=0.904 F1=0.836  B: PR-AUC=0.917 F1=0.829\n",
      "[A6] epoch 18 train_loss=0.1469 val_loss=0.9937 A: PR-AUC=0.913 F1=0.844  B: PR-AUC=0.917 F1=0.844\n",
      "[A6] epoch 19 train_loss=0.1286 val_loss=0.9884 A: PR-AUC=0.918 F1=0.844  B: PR-AUC=0.927 F1=0.861\n",
      "[A6] epoch 20 train_loss=0.1065 val_loss=1.0377 A: PR-AUC=0.917 F1=0.855  B: PR-AUC=0.921 F1=0.863\n",
      "[A7] epoch 1 train_loss=0.4852 val_loss=1.2633 A: PR-AUC=0.819 F1=0.619  B: PR-AUC=0.830 F1=0.631\n",
      "[A7] epoch 2 train_loss=0.4280 val_loss=1.1807 A: PR-AUC=0.830 F1=0.660  B: PR-AUC=0.842 F1=0.679\n",
      "[A7] epoch 3 train_loss=0.4194 val_loss=1.1894 A: PR-AUC=0.834 F1=0.664  B: PR-AUC=0.847 F1=0.667\n",
      "[A7] epoch 4 train_loss=0.4144 val_loss=1.2050 A: PR-AUC=0.836 F1=0.660  B: PR-AUC=0.850 F1=0.642\n",
      "[A7] epoch 5 train_loss=0.4102 val_loss=1.1476 A: PR-AUC=0.839 F1=0.708  B: PR-AUC=0.855 F1=0.708\n",
      "[A7] epoch 6 train_loss=0.4059 val_loss=1.1538 A: PR-AUC=0.839 F1=0.682  B: PR-AUC=0.858 F1=0.707\n",
      "[A7] epoch 7 train_loss=0.3993 val_loss=1.2330 A: PR-AUC=0.843 F1=0.642  B: PR-AUC=0.862 F1=0.633\n",
      "[A7] epoch 8 train_loss=0.3900 val_loss=1.1494 A: PR-AUC=0.851 F1=0.694  B: PR-AUC=0.867 F1=0.716\n",
      "[A7] epoch 9 train_loss=0.3784 val_loss=1.1307 A: PR-AUC=0.856 F1=0.712  B: PR-AUC=0.877 F1=0.728\n",
      "[A7] epoch 10 train_loss=0.3602 val_loss=1.0968 A: PR-AUC=0.863 F1=0.744  B: PR-AUC=0.882 F1=0.759\n",
      "[A7] epoch 11 train_loss=0.3396 val_loss=1.1356 A: PR-AUC=0.869 F1=0.729  B: PR-AUC=0.887 F1=0.745\n",
      "[A7] epoch 12 train_loss=0.3133 val_loss=1.0123 A: PR-AUC=0.878 F1=0.790  B: PR-AUC=0.899 F1=0.799\n",
      "[A7] epoch 13 train_loss=0.2847 val_loss=1.0120 A: PR-AUC=0.886 F1=0.804  B: PR-AUC=0.902 F1=0.807\n",
      "[A7] epoch 14 train_loss=0.2527 val_loss=0.9734 A: PR-AUC=0.897 F1=0.819  B: PR-AUC=0.914 F1=0.813\n",
      "[A7] epoch 15 train_loss=0.2244 val_loss=0.9807 A: PR-AUC=0.902 F1=0.812  B: PR-AUC=0.917 F1=0.827\n",
      "[A7] epoch 16 train_loss=0.1906 val_loss=0.9556 A: PR-AUC=0.899 F1=0.840  B: PR-AUC=0.918 F1=0.848\n",
      "[A7] epoch 17 train_loss=0.1677 val_loss=0.9572 A: PR-AUC=0.912 F1=0.845  B: PR-AUC=0.923 F1=0.846\n",
      "[A7] epoch 18 train_loss=0.1419 val_loss=0.9553 A: PR-AUC=0.914 F1=0.847  B: PR-AUC=0.923 F1=0.852\n",
      "[A7] epoch 19 train_loss=0.1248 val_loss=0.9723 A: PR-AUC=0.912 F1=0.858  B: PR-AUC=0.928 F1=0.858\n",
      "[A7] epoch 20 train_loss=0.1052 val_loss=0.9875 A: PR-AUC=0.914 F1=0.864  B: PR-AUC=0.929 F1=0.867\n",
      "[A7] epoch 21 train_loss=0.0855 val_loss=1.1129 A: PR-AUC=0.916 F1=0.850  B: PR-AUC=0.929 F1=0.860\n",
      "[A7] epoch 22 train_loss=0.0814 val_loss=1.0986 A: PR-AUC=0.924 F1=0.865  B: PR-AUC=0.929 F1=0.854\n",
      "[A7] epoch 23 train_loss=0.0674 val_loss=1.1235 A: PR-AUC=0.921 F1=0.862  B: PR-AUC=0.930 F1=0.866\n",
      "üõë Early stopping at epoch 23\n",
      "[A8] epoch 1 train_loss=0.4882 val_loss=1.2212 A: PR-AUC=0.819 F1=0.642  B: PR-AUC=0.826 F1=0.645\n",
      "[A8] epoch 2 train_loss=0.4271 val_loss=1.2030 A: PR-AUC=0.830 F1=0.628  B: PR-AUC=0.843 F1=0.638\n",
      "[A8] epoch 3 train_loss=0.4197 val_loss=1.1963 A: PR-AUC=0.832 F1=0.652  B: PR-AUC=0.847 F1=0.648\n",
      "[A8] epoch 4 train_loss=0.4150 val_loss=1.1813 A: PR-AUC=0.835 F1=0.670  B: PR-AUC=0.849 F1=0.664\n",
      "[A8] epoch 5 train_loss=0.4106 val_loss=1.1830 A: PR-AUC=0.838 F1=0.670  B: PR-AUC=0.852 F1=0.665\n",
      "[A8] epoch 6 train_loss=0.4063 val_loss=1.2070 A: PR-AUC=0.844 F1=0.646  B: PR-AUC=0.854 F1=0.632\n",
      "[A8] epoch 7 train_loss=0.3994 val_loss=1.1849 A: PR-AUC=0.844 F1=0.692  B: PR-AUC=0.858 F1=0.658\n",
      "[A8] epoch 8 train_loss=0.3902 val_loss=1.1420 A: PR-AUC=0.850 F1=0.727  B: PR-AUC=0.862 F1=0.719\n",
      "[A8] epoch 9 train_loss=0.3804 val_loss=1.1023 A: PR-AUC=0.856 F1=0.741  B: PR-AUC=0.872 F1=0.765\n",
      "[A8] epoch 10 train_loss=0.3653 val_loss=1.1610 A: PR-AUC=0.856 F1=0.729  B: PR-AUC=0.873 F1=0.710\n",
      "[A8] epoch 11 train_loss=0.3462 val_loss=1.0787 A: PR-AUC=0.868 F1=0.770  B: PR-AUC=0.881 F1=0.753\n",
      "[A8] epoch 12 train_loss=0.3221 val_loss=1.0580 A: PR-AUC=0.872 F1=0.785  B: PR-AUC=0.890 F1=0.774\n",
      "[A8] epoch 13 train_loss=0.2952 val_loss=1.0798 A: PR-AUC=0.883 F1=0.765  B: PR-AUC=0.895 F1=0.775\n",
      "[A8] epoch 14 train_loss=0.2658 val_loss=0.9971 A: PR-AUC=0.890 F1=0.809  B: PR-AUC=0.902 F1=0.812\n",
      "[A8] epoch 15 train_loss=0.2359 val_loss=0.9734 A: PR-AUC=0.907 F1=0.822  B: PR-AUC=0.914 F1=0.815\n",
      "[A8] epoch 16 train_loss=0.2088 val_loss=0.9726 A: PR-AUC=0.905 F1=0.819  B: PR-AUC=0.912 F1=0.844\n",
      "[A8] epoch 17 train_loss=0.1801 val_loss=0.9705 A: PR-AUC=0.911 F1=0.838  B: PR-AUC=0.914 F1=0.844\n",
      "[A8] epoch 18 train_loss=0.1524 val_loss=0.9699 A: PR-AUC=0.906 F1=0.850  B: PR-AUC=0.919 F1=0.851\n",
      "[A8] epoch 19 train_loss=0.1350 val_loss=0.9727 A: PR-AUC=0.917 F1=0.852  B: PR-AUC=0.921 F1=0.863\n",
      "[A8] epoch 20 train_loss=0.1183 val_loss=0.9964 A: PR-AUC=0.918 F1=0.859  B: PR-AUC=0.922 F1=0.861\n",
      "[A8] epoch 21 train_loss=0.1010 val_loss=0.9974 A: PR-AUC=0.920 F1=0.867  B: PR-AUC=0.924 F1=0.867\n",
      "[A8] epoch 22 train_loss=0.0794 val_loss=1.0545 A: PR-AUC=0.923 F1=0.872  B: PR-AUC=0.925 F1=0.860\n",
      "[A8] epoch 23 train_loss=0.0726 val_loss=1.0764 A: PR-AUC=0.922 F1=0.863  B: PR-AUC=0.925 F1=0.866\n",
      "üõë Early stopping at epoch 23\n",
      "[A9] epoch 1 train_loss=0.0715 val_loss=1.3975 A: PR-AUC=0.786 F1=0.517  B: PR-AUC=0.802 F1=0.489\n",
      "[A9] epoch 2 train_loss=0.0625 val_loss=1.3075 A: PR-AUC=0.828 F1=0.537  B: PR-AUC=0.842 F1=0.549\n",
      "[A9] epoch 3 train_loss=0.0610 val_loss=1.3085 A: PR-AUC=0.832 F1=0.526  B: PR-AUC=0.844 F1=0.531\n",
      "[A9] epoch 4 train_loss=0.0604 val_loss=1.2740 A: PR-AUC=0.838 F1=0.575  B: PR-AUC=0.850 F1=0.594\n",
      "[A9] epoch 5 train_loss=0.0598 val_loss=1.3207 A: PR-AUC=0.840 F1=0.528  B: PR-AUC=0.852 F1=0.524\n",
      "[A9] epoch 6 train_loss=0.0591 val_loss=1.2899 A: PR-AUC=0.844 F1=0.550  B: PR-AUC=0.858 F1=0.569\n",
      "[A9] epoch 7 train_loss=0.0582 val_loss=1.2494 A: PR-AUC=0.845 F1=0.599  B: PR-AUC=0.864 F1=0.636\n",
      "[A9] epoch 8 train_loss=0.0572 val_loss=1.2611 A: PR-AUC=0.850 F1=0.588  B: PR-AUC=0.866 F1=0.602\n",
      "[A9] epoch 9 train_loss=0.0553 val_loss=1.2252 A: PR-AUC=0.857 F1=0.650  B: PR-AUC=0.871 F1=0.664\n",
      "[A9] epoch 10 train_loss=0.0530 val_loss=1.2465 A: PR-AUC=0.865 F1=0.637  B: PR-AUC=0.883 F1=0.612\n",
      "[A9] epoch 11 train_loss=0.0498 val_loss=1.1924 A: PR-AUC=0.874 F1=0.705  B: PR-AUC=0.886 F1=0.670\n",
      "[A9] epoch 12 train_loss=0.0460 val_loss=1.1653 A: PR-AUC=0.888 F1=0.724  B: PR-AUC=0.899 F1=0.695\n",
      "[A9] epoch 13 train_loss=0.0413 val_loss=1.0818 A: PR-AUC=0.894 F1=0.764  B: PR-AUC=0.907 F1=0.783\n",
      "[A9] epoch 14 train_loss=0.0362 val_loss=1.0539 A: PR-AUC=0.907 F1=0.759  B: PR-AUC=0.913 F1=0.786\n",
      "[A9] epoch 15 train_loss=0.0312 val_loss=1.0037 A: PR-AUC=0.911 F1=0.770  B: PR-AUC=0.914 F1=0.799\n",
      "[A9] epoch 16 train_loss=0.0269 val_loss=0.9578 A: PR-AUC=0.913 F1=0.824  B: PR-AUC=0.922 F1=0.801\n",
      "[A9] epoch 17 train_loss=0.0225 val_loss=0.9029 A: PR-AUC=0.920 F1=0.835  B: PR-AUC=0.930 F1=0.829\n",
      "[A9] epoch 18 train_loss=0.0183 val_loss=0.8650 A: PR-AUC=0.926 F1=0.846  B: PR-AUC=0.926 F1=0.840\n",
      "[A9] epoch 19 train_loss=0.0157 val_loss=0.8122 A: PR-AUC=0.930 F1=0.855  B: PR-AUC=0.934 F1=0.853\n",
      "[A9] epoch 20 train_loss=0.0139 val_loss=0.7798 A: PR-AUC=0.933 F1=0.865  B: PR-AUC=0.933 F1=0.867\n",
      "[A9] epoch 21 train_loss=0.0111 val_loss=0.7995 A: PR-AUC=0.931 F1=0.865  B: PR-AUC=0.931 F1=0.850\n",
      "[A9] epoch 22 train_loss=0.0107 val_loss=0.7842 A: PR-AUC=0.930 F1=0.871  B: PR-AUC=0.933 F1=0.858\n",
      "[A9] epoch 23 train_loss=0.0096 val_loss=0.7873 A: PR-AUC=0.931 F1=0.865  B: PR-AUC=0.937 F1=0.862\n",
      "[A9] epoch 24 train_loss=0.0072 val_loss=0.7738 A: PR-AUC=0.935 F1=0.866  B: PR-AUC=0.936 F1=0.872\n",
      "[A9] epoch 25 train_loss=0.0061 val_loss=0.7872 A: PR-AUC=0.933 F1=0.873  B: PR-AUC=0.936 F1=0.860\n",
      "[A10] epoch 1 train_loss=0.4464 val_loss=1.1853 A: PR-AUC=0.838 F1=0.642  B: PR-AUC=0.851 F1=0.675\n",
      "[A10] epoch 2 train_loss=0.4105 val_loss=1.1887 A: PR-AUC=0.844 F1=0.649  B: PR-AUC=0.857 F1=0.661\n",
      "[A10] epoch 3 train_loss=0.4040 val_loss=1.1737 A: PR-AUC=0.845 F1=0.656  B: PR-AUC=0.861 F1=0.689\n",
      "[A10] epoch 4 train_loss=0.3983 val_loss=1.1537 A: PR-AUC=0.849 F1=0.676  B: PR-AUC=0.863 F1=0.724\n",
      "[A10] epoch 5 train_loss=0.3915 val_loss=1.1535 A: PR-AUC=0.852 F1=0.700  B: PR-AUC=0.867 F1=0.696\n",
      "[A10] epoch 6 train_loss=0.3848 val_loss=1.1001 A: PR-AUC=0.854 F1=0.748  B: PR-AUC=0.869 F1=0.749\n",
      "[A10] epoch 7 train_loss=0.3781 val_loss=1.1337 A: PR-AUC=0.853 F1=0.713  B: PR-AUC=0.875 F1=0.731\n",
      "[A10] epoch 8 train_loss=0.3709 val_loss=1.1114 A: PR-AUC=0.858 F1=0.729  B: PR-AUC=0.874 F1=0.749\n",
      "[A10] epoch 9 train_loss=0.3637 val_loss=1.0814 A: PR-AUC=0.860 F1=0.738  B: PR-AUC=0.879 F1=0.778\n",
      "[A10] epoch 10 train_loss=0.3577 val_loss=1.1170 A: PR-AUC=0.864 F1=0.721  B: PR-AUC=0.881 F1=0.741\n",
      "[A10] epoch 11 train_loss=0.3508 val_loss=1.0883 A: PR-AUC=0.866 F1=0.739  B: PR-AUC=0.884 F1=0.772\n",
      "[A10] epoch 12 train_loss=0.3435 val_loss=1.1056 A: PR-AUC=0.869 F1=0.763  B: PR-AUC=0.882 F1=0.730\n",
      "[A10] epoch 13 train_loss=0.3374 val_loss=1.1543 A: PR-AUC=0.872 F1=0.766  B: PR-AUC=0.887 F1=0.684\n",
      "[A10] epoch 14 train_loss=0.3318 val_loss=1.1422 A: PR-AUC=0.870 F1=0.700  B: PR-AUC=0.889 F1=0.767\n",
      "üõë Early stopping at epoch 14\n",
      "[A4] epoch 1 train_loss=0.4852 val_loss=1.2322 A: PR-AUC=0.814 F1=0.595  B: PR-AUC=0.828 F1=0.609\n",
      "[A4] epoch 2 train_loss=0.4273 val_loss=1.2282 A: PR-AUC=0.827 F1=0.609  B: PR-AUC=0.843 F1=0.621\n",
      "[A4] epoch 3 train_loss=0.4189 val_loss=1.1999 A: PR-AUC=0.834 F1=0.639  B: PR-AUC=0.848 F1=0.636\n",
      "[A4] epoch 4 train_loss=0.4148 val_loss=1.1901 A: PR-AUC=0.836 F1=0.649  B: PR-AUC=0.850 F1=0.662\n",
      "[A4] epoch 5 train_loss=0.4109 val_loss=1.1564 A: PR-AUC=0.838 F1=0.681  B: PR-AUC=0.854 F1=0.719\n",
      "[A4] epoch 6 train_loss=0.4059 val_loss=1.1409 A: PR-AUC=0.843 F1=0.707  B: PR-AUC=0.857 F1=0.725\n",
      "[A4] epoch 7 train_loss=0.3987 val_loss=1.1793 A: PR-AUC=0.844 F1=0.666  B: PR-AUC=0.863 F1=0.682\n",
      "[A4] epoch 8 train_loss=0.3898 val_loss=1.1447 A: PR-AUC=0.847 F1=0.690  B: PR-AUC=0.868 F1=0.729\n",
      "[A4] epoch 9 train_loss=0.3795 val_loss=1.1123 A: PR-AUC=0.850 F1=0.726  B: PR-AUC=0.871 F1=0.748\n",
      "[A4] epoch 10 train_loss=0.3655 val_loss=1.1202 A: PR-AUC=0.858 F1=0.724  B: PR-AUC=0.876 F1=0.740\n",
      "[A4] epoch 11 train_loss=0.3477 val_loss=1.1075 A: PR-AUC=0.865 F1=0.730  B: PR-AUC=0.886 F1=0.774\n",
      "[A4] epoch 12 train_loss=0.3260 val_loss=1.1359 A: PR-AUC=0.868 F1=0.734  B: PR-AUC=0.890 F1=0.749\n",
      "[A4] epoch 13 train_loss=0.3003 val_loss=1.0578 A: PR-AUC=0.872 F1=0.782  B: PR-AUC=0.896 F1=0.794\n",
      "[A4] epoch 14 train_loss=0.2721 val_loss=1.0295 A: PR-AUC=0.884 F1=0.803  B: PR-AUC=0.903 F1=0.792\n",
      "[A4] epoch 15 train_loss=0.2447 val_loss=0.9976 A: PR-AUC=0.888 F1=0.803  B: PR-AUC=0.910 F1=0.824\n",
      "[A4] epoch 16 train_loss=0.2167 val_loss=1.0066 A: PR-AUC=0.896 F1=0.816  B: PR-AUC=0.914 F1=0.832\n",
      "[A4] epoch 17 train_loss=0.1848 val_loss=0.9833 A: PR-AUC=0.903 F1=0.837  B: PR-AUC=0.915 F1=0.838\n",
      "[A4] epoch 18 train_loss=0.1606 val_loss=0.9761 A: PR-AUC=0.908 F1=0.848  B: PR-AUC=0.923 F1=0.849\n",
      "[A4] epoch 19 train_loss=0.1353 val_loss=0.9775 A: PR-AUC=0.912 F1=0.855  B: PR-AUC=0.917 F1=0.849\n",
      "[A4] epoch 20 train_loss=0.1171 val_loss=1.0274 A: PR-AUC=0.917 F1=0.849  B: PR-AUC=0.925 F1=0.851\n",
      "[A4] epoch 21 train_loss=0.1053 val_loss=1.0282 A: PR-AUC=0.916 F1=0.852  B: PR-AUC=0.924 F1=0.857\n",
      "[A4] epoch 22 train_loss=0.0890 val_loss=1.0950 A: PR-AUC=0.912 F1=0.858  B: PR-AUC=0.924 F1=0.860\n",
      "[A4] epoch 23 train_loss=0.0701 val_loss=1.0815 A: PR-AUC=0.920 F1=0.859  B: PR-AUC=0.925 F1=0.874\n",
      "üõë Early stopping at epoch 23\n",
      "Saved results to: out/DBRA24_results.xlsx out/DBRA24_results.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>precision_auc_A</th>\n",
       "      <th>f1_A</th>\n",
       "      <th>precision_auc_B</th>\n",
       "      <th>rmse_C</th>\n",
       "      <th>ece</th>\n",
       "      <th>ttd_s</th>\n",
       "      <th>fp_per_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GBM (A1)</td>\n",
       "      <td>0.982549</td>\n",
       "      <td>0.958790</td>\n",
       "      <td>0.985423</td>\n",
       "      <td>0.033293</td>\n",
       "      <td>0.064044</td>\n",
       "      <td>0.161149</td>\n",
       "      <td>30.885329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bi-LSTM (Full)</td>\n",
       "      <td>0.947470</td>\n",
       "      <td>0.900970</td>\n",
       "      <td>0.944539</td>\n",
       "      <td>0.064036</td>\n",
       "      <td>0.050045</td>\n",
       "      <td>0.452822</td>\n",
       "      <td>68.752108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bi-LSTM (No Context)</td>\n",
       "      <td>0.954274</td>\n",
       "      <td>0.914124</td>\n",
       "      <td>0.954677</td>\n",
       "      <td>0.065887</td>\n",
       "      <td>0.041885</td>\n",
       "      <td>0.363186</td>\n",
       "      <td>61.618887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bi-LSTM (No Gating)</td>\n",
       "      <td>0.785460</td>\n",
       "      <td>0.839136</td>\n",
       "      <td>0.789807</td>\n",
       "      <td>0.061519</td>\n",
       "      <td>0.101538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>199.502530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bi-LSTM (Lag)</td>\n",
       "      <td>0.950832</td>\n",
       "      <td>0.908777</td>\n",
       "      <td>0.956069</td>\n",
       "      <td>0.063429</td>\n",
       "      <td>0.088647</td>\n",
       "      <td>0.530844</td>\n",
       "      <td>54.713322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bi-LSTM (Short Seq)</td>\n",
       "      <td>0.948769</td>\n",
       "      <td>0.903887</td>\n",
       "      <td>0.952976</td>\n",
       "      <td>0.064452</td>\n",
       "      <td>0.080681</td>\n",
       "      <td>0.416424</td>\n",
       "      <td>68.145025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bi-LSTM (Lag Robust)</td>\n",
       "      <td>0.959205</td>\n",
       "      <td>0.924437</td>\n",
       "      <td>0.962469</td>\n",
       "      <td>0.063893</td>\n",
       "      <td>0.035853</td>\n",
       "      <td>0.339799</td>\n",
       "      <td>52.133221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bi-LSTM (MapFree)</td>\n",
       "      <td>0.954255</td>\n",
       "      <td>0.913759</td>\n",
       "      <td>0.955402</td>\n",
       "      <td>0.064097</td>\n",
       "      <td>0.025148</td>\n",
       "      <td>0.456468</td>\n",
       "      <td>56.003373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bi-LSTM (Focal)</td>\n",
       "      <td>0.972366</td>\n",
       "      <td>0.950514</td>\n",
       "      <td>0.968369</td>\n",
       "      <td>0.060687</td>\n",
       "      <td>0.078379</td>\n",
       "      <td>0.230421</td>\n",
       "      <td>34.072513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>TCN Small</td>\n",
       "      <td>0.863947</td>\n",
       "      <td>0.845501</td>\n",
       "      <td>0.862240</td>\n",
       "      <td>0.066423</td>\n",
       "      <td>0.114199</td>\n",
       "      <td>0.316465</td>\n",
       "      <td>150.480607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Bi-LSTM (Big)</td>\n",
       "      <td>0.958925</td>\n",
       "      <td>0.926037</td>\n",
       "      <td>0.958108</td>\n",
       "      <td>0.064834</td>\n",
       "      <td>0.057110</td>\n",
       "      <td>0.366049</td>\n",
       "      <td>49.325464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   model  precision_auc_A      f1_A  precision_auc_B  \\\n",
       "0               GBM (A1)         0.982549  0.958790         0.985423   \n",
       "1         Bi-LSTM (Full)         0.947470  0.900970         0.944539   \n",
       "2   Bi-LSTM (No Context)         0.954274  0.914124         0.954677   \n",
       "3    Bi-LSTM (No Gating)         0.785460  0.839136         0.789807   \n",
       "4          Bi-LSTM (Lag)         0.950832  0.908777         0.956069   \n",
       "5    Bi-LSTM (Short Seq)         0.948769  0.903887         0.952976   \n",
       "6   Bi-LSTM (Lag Robust)         0.959205  0.924437         0.962469   \n",
       "7      Bi-LSTM (MapFree)         0.954255  0.913759         0.955402   \n",
       "8        Bi-LSTM (Focal)         0.972366  0.950514         0.968369   \n",
       "9              TCN Small         0.863947  0.845501         0.862240   \n",
       "10         Bi-LSTM (Big)         0.958925  0.926037         0.958108   \n",
       "\n",
       "      rmse_C       ece     ttd_s  fp_per_hour  \n",
       "0   0.033293  0.064044  0.161149    30.885329  \n",
       "1   0.064036  0.050045  0.452822    68.752108  \n",
       "2   0.065887  0.041885  0.363186    61.618887  \n",
       "3   0.061519  0.101538  0.000000   199.502530  \n",
       "4   0.063429  0.088647  0.530844    54.713322  \n",
       "5   0.064452  0.080681  0.416424    68.145025  \n",
       "6   0.063893  0.035853  0.339799    52.133221  \n",
       "7   0.064097  0.025148  0.456468    56.003373  \n",
       "8   0.060687  0.078379  0.230421    34.072513  \n",
       "9   0.066423  0.114199  0.316465   150.480607  \n",
       "10  0.064834  0.057110  0.366049    49.325464  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ================== Ablations (A2‚ÄìA10) ==================\n",
    "results=[dict(model=\"GBM (A1)\",**res_A1), dict(model=\"Bi-LSTM (Full)\",**met_full)]\n",
    "order=[\"A2\",\"A3\",\"A5\",\"A6\",\"A7\",\"A8\",\"A9\",\"A10\",\"A4\"]\n",
    "name_map={\"A2\":\"Bi-LSTM (No Context)\",\"A3\":\"Bi-LSTM (No Gating)\",\"A5\":\"Bi-LSTM (Lag)\",\"A6\":\"Bi-LSTM (Short Seq)\",\n",
    "          \"A7\":\"Bi-LSTM (Lag Robust)\",\"A8\":\"Bi-LSTM (MapFree)\",\"A9\":\"Bi-LSTM (Focal)\",\"A10\":\"TCN Small\",\"A4\":\"Bi-LSTM (Big)\"}\n",
    "for code in order:\n",
    "    try:\n",
    "        # TODO: insert your seq rebuilding logic here if features differ\n",
    "        mdl = BiLSTM(X_train.shape[-1],S_train.shape[-1],hidden=CFG['HIDDEN'],gated=True) if code!=\"A10\" else TCN(X_train.shape[-1],S_train.shape[-1],ch=CFG['TCN_CHANNELS'])\n",
    "        mdl=train_model(mdl,code,\"focal\" if code==\"A9\" else \"bce_weighted\",dl_train,dl_val,yA_train,yB_train)\n",
    "        pA,pB,pC=predict_model(mdl,ds_test); m=eval_all(code,yA_test,yB_test,yC_test,pA,pB,pC,test['meta'])\n",
    "        results.append(dict(model=name_map[code],**m))\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {code} failed: {e}\")\n",
    "        results.append(dict(model=name_map[code],precision_auc_A=np.nan,f1_A=np.nan,precision_auc_B=np.nan,rmse_C=np.nan,\n",
    "                            ece=np.nan,ttd_s=np.nan,fp_per_hour=np.nan))\n",
    "\n",
    "# ================== Save Results ==================\n",
    "df_res=pd.DataFrame(results)\n",
    "excel_path, csv_path = OUT_DIR/\"DBRA24_results.xlsx\", OUT_DIR/\"DBRA24_results.csv\"\n",
    "df_res.to_csv(csv_path,index=False); df_res.to_excel(excel_path,index=False)\n",
    "print(\"Saved results to:\",excel_path,csv_path)\n",
    "display(df_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2636a5eb",
   "metadata": {},
   "source": [
    "## 13) üìä Export Results (Excel, CSV, Plots) + Final Comparison Table\n",
    "<span style=\"background:#FFF8E1;color:#B37400;padding:6px 10px;border-radius:6px;display:inline-block;\">\n",
    "All artifacts are saved under <code>/kaggle/working/</code>.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2be07f35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T17:47:42.464834Z",
     "iopub.status.busy": "2025-10-08T17:47:42.464099Z",
     "iopub.status.idle": "2025-10-08T17:47:42.490506Z",
     "shell.execute_reply": "2025-10-08T17:47:42.489921Z",
     "shell.execute_reply.started": "2025-10-08T17:47:42.464808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Excel  -> out/DBRA24_results.xlsx\n",
      "Saved CSV    -> out/DBRA24_results.csv\n",
      "\n",
      "### Final Comparison Table (ready to paste)\n",
      "\n",
      "| Model / Ablation | PR-AUC (A) | F1 (A) | PR-AUC (B) | RMSE (C) | ECE | TTD (s) ‚Üì | FP/h ‚Üì |\n",
      "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
      "| GBM (A1) | 0.983 | 0.959 | 0.985 | 0.0333 | 0.064 | 0.161 | 30.9 |\n",
      "| Bi-LSTM (Full) | 0.947 | 0.901 | 0.945 | 0.064 | 0.05 | 0.453 | 68.8 |\n",
      "| Bi-LSTM (No Context) | 0.954 | 0.914 | 0.955 | 0.0659 | 0.0419 | 0.363 | 61.6 |\n",
      "| Bi-LSTM (No Gating) | 0.785 | 0.839 | 0.79 | 0.0615 | 0.102 | 0 | 200 |\n",
      "| Bi-LSTM (Lag) | 0.951 | 0.909 | 0.956 | 0.0634 | 0.0886 | 0.531 | 54.7 |\n",
      "| Bi-LSTM (Short Seq) | 0.949 | 0.904 | 0.953 | 0.0645 | 0.0807 | 0.416 | 68.1 |\n",
      "| Bi-LSTM (Lag Robust) | 0.959 | 0.924 | 0.962 | 0.0639 | 0.0359 | 0.34 | 52.1 |\n",
      "| Bi-LSTM (MapFree) | 0.954 | 0.914 | 0.955 | 0.0641 | 0.0251 | 0.456 | 56 |\n",
      "| Bi-LSTM (Focal) | 0.972 | 0.951 | 0.968 | 0.0607 | 0.0784 | 0.23 | 34.1 |\n",
      "| TCN Small | 0.864 | 0.846 | 0.862 | 0.0664 | 0.114 | 0.316 | 150 |\n",
      "| Bi-LSTM (Big) | 0.959 | 0.926 | 0.958 | 0.0648 | 0.0571 | 0.366 | 49.3 |\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Results Saving (Clean Version)\n",
    "# ===============================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Expected columns mapping (only those you *might* compute)\n",
    "final_cols = {\n",
    "    'model'        : 'Model / Ablation',\n",
    "    'precision_auc_A': 'PR-AUC (A)',\n",
    "    'f1_A'         : 'F1 (A)',\n",
    "    'precision_auc_B': 'PR-AUC (B)',\n",
    "    'rmse_C'       : 'RMSE (C)',\n",
    "    'ece'          : 'ECE',\n",
    "    'ttd_s'        : 'TTD (s) ‚Üì',\n",
    "    'fp_per_hour'  : 'FP/h ‚Üì',\n",
    "    'latency_ms'   : 'Latency (ms) ‚Üì',\n",
    "    'flops_m'      : 'FLOPs (M) ‚Üì'\n",
    "}\n",
    "\n",
    "# ‚úÖ Only keep the ones that exist in df_res\n",
    "available_cols = {k:v for k,v in final_cols.items() if k in df_res.columns}\n",
    "\n",
    "# Rename + reorder\n",
    "df_final = df_res.rename(columns=available_cols)\n",
    "df_final = df_final[[available_cols[k] for k in available_cols]]\n",
    "\n",
    "# Save Excel + CSV\n",
    "excel_path = OUT_DIR/\"DBRA24_results.xlsx\"\n",
    "csv_path   = OUT_DIR/\"DBRA24_results.csv\"\n",
    "\n",
    "with pd.ExcelWriter(excel_path, engine=\"openpyxl\") as writer:\n",
    "    df_final.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "    df_res.to_excel(writer, sheet_name=\"Raw\", index=False)\n",
    "\n",
    "df_final.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"Saved Excel  -> {excel_path}\")\n",
    "print(f\"Saved CSV    -> {csv_path}\")\n",
    "\n",
    "# Markdown table for paper\n",
    "def to_md_table(df):\n",
    "    hdr = \"| \" + \" | \".join(df.columns) + \" |\"\n",
    "    sep = \"| \" + \" | \".join([\"---\"]*len(df.columns)) + \" |\"\n",
    "    rows = [\"| \" + \" | \".join(f\"{x:.3g}\" if isinstance(x,(int,float,np.floating)) else str(x) for x in r) + \" |\" for r in df.values]\n",
    "    return \"\\n\".join([hdr, sep] + rows)\n",
    "\n",
    "print(\"\\n### Final Comparison Table (ready to paste)\\n\")\n",
    "print(to_md_table(df_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4119fd94-22ff-4a1f-82e6-8060672c531b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T17:47:42.491563Z",
     "iopub.status.busy": "2025-10-08T17:47:42.491355Z",
     "iopub.status.idle": "2025-10-08T17:47:46.592883Z",
     "shell.execute_reply": "2025-10-08T17:47:46.592249Z",
     "shell.execute_reply.started": "2025-10-08T17:47:42.491549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Zipped all outputs to /kaggle/working/output_results.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil, os\n",
    "\n",
    "# Path where Kaggle saves outputs\n",
    "OUT_DIR = \"/kaggle/working\"\n",
    "\n",
    "# Zip file name\n",
    "zip_path = \"/kaggle/working/output_results.zip\"\n",
    "\n",
    "# Remove old zip if exists\n",
    "if os.path.exists(zip_path):\n",
    "    os.remove(zip_path)\n",
    "\n",
    "# Create new zip (recursively includes all files in working dir)\n",
    "shutil.make_archive(zip_path.replace(\".zip\",\"\"), 'zip', OUT_DIR)\n",
    "\n",
    "print(f\"‚úÖ Zipped all outputs to {zip_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5735160,
     "sourceId": 9438298,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
